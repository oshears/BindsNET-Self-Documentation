{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# BindsNET Learning Techniques\n",
    "\n",
    "## 1. Table of Contents\n",
    "1. Table of Contents\n",
    "2. Overview\n",
    "3. Import Statements\n",
    "4. Learning Flow\n",
    "    1. Define Simulation Parameters\n",
    "    2. Create Input Data\n",
    "    3. Configure Network Architecture\n",
    "    4. Define Simulation Variables\n",
    "    5. Perform Learning Iterations\n",
    "    6. Evaluate Classification Performance\n",
    "5. Learning Rules\n",
    "    1. PostPre\n",
    "    2. Hebbian\n",
    "    3. WeightDependentPostPre\n",
    "    4. MSTDP\n",
    "    5. MSTDPET\n",
    "6. Custom Learning Rules\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Overview\n",
    "\n",
    "Detail documentation of usage of learning rules has been specified [here](https://bindsnet-docs.readthedocs.io/guide/guide_part_ii.html). This document will go into more specific examples of configuring a spiking neural network in BindsNET."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The specified learning rule is passed into a `Connection` object via the `update_rule` argument. The connection encapsulates the learning rule object.\n",
    "\n",
    "* `nu`: a 2-tuple pre- and post- synaptic learning rates (how quickly synapse weights change)\n",
    "* `reduction`: specifies how parameter updates are aggregated across the batch dimension\n",
    "* `weight_decay`: specifies the time constant of the rate of decay of synapse weights to zero\n",
    "\n",
    "Parameter updates are averaged across the batch dimension by default, so there is no weight decay.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from bindsnet.network.nodes import Input, LIFNodes\n",
    "from bindsnet.network.topology import Connection\n",
    "from bindsnet.learning import PostPre\n",
    "\n",
    "# Create two populations of neurons, one to act as the \"source\"\n",
    "# population, and the other, the \"target population\".\n",
    "# Neurons involved in certain learning rules must record synaptic\n",
    "# traces, a vector of short-term memories of the last emitted spikes.\n",
    "source_layer = Input(n=100, traces=True)\n",
    "target_layer = LIFNodes(n=1000, traces=True)\n",
    "\n",
    "# Connect the two layers.\n",
    "connection = Connection(\n",
    "    source=source_layer, target=target_layer, update_rule=PostPre, nu=(1e-4, 1e-2)\n",
    ")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 382,
   "outputs": []
  },
  {
   "source": [
    "## 3. Import Statements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from bindsnet.encoding import *\n",
    "from bindsnet.network import Network\n",
    "from bindsnet.network.monitors import Monitor\n",
    "from bindsnet.network.monitors import NetworkMonitor\n",
    "\n",
    "from bindsnet.analysis.plotting import plot_spikes, plot_voltages, plot_input, plot_weights\n",
    "\n",
    "from bindsnet.network.nodes import Input, LIFNodes\n",
    "from bindsnet.network.topology import Connection\n",
    "from bindsnet.learning import PostPre, Hebbian, WeightDependentPostPre, MSTDP, MSTDPET\n",
    "\n",
    "from bindsnet.evaluation import all_activity, proportion_weighting, assign_labels\n",
    "from bindsnet.utils import get_square_weights, get_square_assignments"
   ]
  },
  {
   "source": [
    "## 4. Learning Flow\n",
    "\n",
    "1. Define Simulation Parameters\n",
    "2. Create Input Data\n",
    "3. Configure Network Architecture\n",
    "4. Define Simulation Variables\n",
    "5. Perform Learning Iterations\n",
    "6. Evaluate Classification Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.1 Simulation Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input Data Parameters ###\n",
    "\n",
    "# number of training samples\n",
    "training_samples = 10\n",
    "testing_samples = 10\n",
    "\n",
    "# set number of classes\n",
    "n_classes = 2\n",
    "\n",
    "### Network Configuration Parameters ###\n",
    "\n",
    "# configure number of input neurons\n",
    "input_layer_name = \"Input Layer\"\n",
    "input_neurons = 9\n",
    "\n",
    "# configure the number of output lif neurons\n",
    "lif_layer_name = \"LIF Layer\"\n",
    "lif_neurons = 2\n",
    "\n",
    "### Simulation Parameters ###\n",
    "\n",
    "# simulation time\n",
    "time = 10\n",
    "dt = 1\n",
    "\n",
    "# number of training iterations\n",
    "epochs = 100\n",
    "\n",
    "# ratio of neurons to classes\n",
    "per_class = int(lif_neurons / n_classes)"
   ]
  },
  {
   "source": [
    "### 4.2 Input Configuration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store unique images in a list\n",
    "imgs = []\n",
    "\n",
    "# Class 0 Image\n",
    "img0 = {\"Label\" : 0, \"Image\" : torch.FloatTensor([[1,1,1],[1,0,1],[1,1,1]])}\n",
    "imgs.append(img0)\n",
    "\n",
    "# Class 1 Image\n",
    "img1 = {\"Label\" : 1, \"Image\" : torch.FloatTensor([[0,1,0],[0,1,0],[0,1,0]])}\n",
    "imgs.append(img1)\n",
    "\n",
    "# initialize list of inputs for training\n",
    "training_dataset = []\n",
    "\n",
    "# for the number of specified training samples\n",
    "for i in range(training_samples):\n",
    "\n",
    "    # randomly select a training sample\n",
    "    # rand_sample = random.randint(0,n_classes-1)\n",
    "    \n",
    "    # provide an even number of training samples\n",
    "    rand_sample = i % n_classes\n",
    "\n",
    "    # add the sample to the list of training samples\n",
    "    training_dataset.append(imgs[rand_sample])\n",
    "\n",
    "# initialize the encoder\n",
    "encoder = BernoulliEncoder(time=time, dt=dt)\n",
    "\n",
    "# list of encoded images for random selection during training\n",
    "encoded_train_inputs = []\n",
    "\n",
    "# loop through encode each image type and store into a list of encoded images\n",
    "for sample in training_dataset:\n",
    "\n",
    "    # encode the image \n",
    "    encoded_img = encoder(torch.flatten(sample[\"Image\"]))\n",
    "\n",
    "    # encoded image input for the network\n",
    "    encoded_img_input = {input_layer_name: encoded_img}\n",
    "\n",
    "    # encoded image label\n",
    "    encoded_img_label = sample[\"Label\"]\n",
    "\n",
    "    # add to the encoded input list along with the input layer name\n",
    "    encoded_train_inputs.append({\"Label\" : encoded_img_label, \"Inputs\" : encoded_img_input})\n",
    "\n",
    "# initialize list of inputs for testing\n",
    "testing_dataset = []\n",
    "\n",
    "# for the number of specified testing samples\n",
    "for i in range(testing_samples):\n",
    "\n",
    "    # randomly select a training sample\n",
    "    rand_sample = random.randint(0,n_classes-1)\n",
    "\n",
    "    # add the sample to the list of training samples\n",
    "    testing_dataset.append(imgs[rand_sample])\n",
    "\n",
    "# list of encoded images for random selection during training\n",
    "encoded_test_inputs = []\n",
    "\n",
    "# loop through encode each image type and store into a list of encoded images\n",
    "for sample in testing_dataset:\n",
    "\n",
    "    # encode the image \n",
    "    encoded_img = encoder(torch.flatten(sample[\"Image\"]))\n",
    "\n",
    "    # encoded image input for the network\n",
    "    encoded_img_input = {input_layer_name: encoded_img}\n",
    "\n",
    "    # encoded image label\n",
    "    encoded_img_label = sample[\"Label\"]\n",
    "\n",
    "    # add to the encoded input list along with the input layer name\n",
    "    encoded_test_inputs.append({\"Label\" : encoded_img_label, \"Inputs\" : encoded_img_input})"
   ]
  },
  {
   "source": [
    "### 4.3 Network Configuration\n",
    "\n",
    "When creating a connection between two layers, the learning (update) rule should be specified as well as the learning rate (nu) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network\n",
    "network = Network()\n",
    "\n",
    "# configure weights for the synapses between the input layer and LIF layer\n",
    "w = torch.round(torch.abs(2 * torch.randn(input_neurons, lif_neurons)))\n",
    "\n",
    "# initialize input and LIF layers\n",
    "# spike traces must be recorded (why?)\n",
    "\n",
    "# initialize input layer\n",
    "input_layer = Input(n=input_neurons,traces=True)\n",
    "\n",
    "# initialize input layer\n",
    "lif_layer = LIFNodes(n=lif_neurons,traces=True)\n",
    "\n",
    "# initialize connection between the input layer and the LIF layer\n",
    "# specify the learning (update) rule and learning rate (nu)\n",
    "connection = Connection(\n",
    "    source=input_layer, target=lif_layer, w=w, update_rule=PostPre, nu=(1e-4, 1e-2)\n",
    ")\n",
    "\n",
    "# add input layer to the network\n",
    "network.add_layer(\n",
    "    layer=input_layer, name=input_layer_name\n",
    ")\n",
    "\n",
    "# add lif neuron layer to the network\n",
    "network.add_layer(\n",
    "    layer=lif_layer, name=lif_layer_name\n",
    ")\n",
    "\n",
    "# add connection to network\n",
    "network.add_connection(\n",
    "    connection=connection, source=input_layer_name, target=lif_layer_name\n",
    ")"
   ]
  },
  {
   "source": [
    "### 4.4 Simulation Variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the spike times of each neuron during the simulation.\n",
    "spike_record = torch.zeros(1, int(time / dt), lif_neurons)\n",
    "\n",
    "# record the mapping of each neuron to its corresponding label\n",
    "assignments = -torch.ones_like(torch.Tensor(lif_neurons))\n",
    "\n",
    "# how frequently each neuron fires for each input class\n",
    "rates = torch.zeros_like(torch.Tensor(lif_neurons, n_classes))\n",
    "\n",
    "# the likelihood of each neuron firing for each input class\n",
    "proportions = torch.zeros_like(torch.Tensor(lif_neurons, n_classes))\n",
    "\n",
    "\n",
    "# label(s) of the input(s) being processed\n",
    "labels = torch.empty(1,dtype=torch.int)\n",
    "\n",
    "# create a spike monitor for each layer in the network\n",
    "# this allows us to read the spikes in order to assign labels to neurons and determine the predicted class \n",
    "layer_monitors = {}\n",
    "for layer in set(network.layers):\n",
    "\n",
    "    # initialize spike monitor at the layer\n",
    "    # do not record the voltage if at the input layer\n",
    "    state_vars = [\"s\",\"v\"] if (layer != input_layer_name) else [\"s\"]\n",
    "    layer_monitors[layer] = Monitor(network.layers[layer], state_vars=state_vars, time=time)\n",
    "\n",
    "    # connect the monitor to the network\n",
    "    network.add_monitor(layer_monitors[layer], name=\"%s_spikes\" % layer)"
   ]
  },
  {
   "source": [
    "### 4.5 Training\n",
    "\n",
    "Below are descriptions of the functions for evaluating the behavior of an SNN in BindsNET\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "`all_activity()`\n",
    "\n",
    "Classify data with the label with highest average spiking activity over all neurons.\n",
    "\n",
    "Returns a predictions tensor of shape `(n_samples,)` resulting from the \"all activity\" classification scheme (`torch.Tensor`)\n",
    "\n",
    "| Parameter  | Type         | Description                                                                           | Default Value |\n",
    "|-------------|--------------|---------------------------------------------------------------------------------------|---------|\n",
    "| spikes      | `torch.Tensor` | Binary tensor of shape `(n_samples, time, n_neurons)` of a layer'sspiking activity. |         |\n",
    "| assignments | `torch.Tensor` | A vector of shape `(n_neurons,)` of neuron label assignments.                       |         |\n",
    "| n_labels    | `int`          | The number of target labels in the data.                                              |         |\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "`proportion_weighting()`\n",
    "\n",
    "Classify data with the label with highest average spiking activity over all neurons, weighted by class-wise proportion.\n",
    "\n",
    "Returns a predictions tensor of shape `(n_samples,)` resulting from the \"proportion weighting\" classification scheme (`torch.Tensor`)\n",
    "\n",
    "| Parameter   | Type         | Description                                                                                              | Default Value |\n",
    "|-------------|--------------|----------------------------------------------------------------------------------------------------------|---------------|\n",
    "| spikes      | `torch.Tensor` | Binary tensor of shape `(n_samples, time, n_neurons)` of a single layer's spiking activity.            |               |\n",
    "| assignments | `torch.Tensor` | A vector of shape `(n_neurons,)` of neuron label assignments.                                          |               |\n",
    "| proportions | `torch.Tensor` | A matrix of shape `(n_neurons, n_labels)` giving the per-class proportions of neuron spiking activity. |               |\n",
    "| n_labels    | `int`          | The number of target labels in the data.                                                                 |               |\n",
    "\n",
    "----\n",
    "\n",
    "`assign_labels()`\n",
    "\n",
    "Assign labels to the neurons based on highest average spiking activity.\n",
    "\n",
    "Returns a Tuple of class assignments, per-class spike proportions, and per-class firing rates (`Tuple[torch.Tensor, torch.Tensor, torch.Tensor]`)\n",
    "\n",
    "| Parameter | Type                     | Descriptions                                                                                  | Default Value |  \n",
    "|------------|--------------------------|-----------------------------------------------------------------------------------------------|---------------|\n",
    "| spikes     | `torch.Tensor`             | Binary tensor of shape `(n_samples, time, n_neurons)` of a single layer's spiking activity. |                | \n",
    "| labels     | `torch.Tensor`             | Vector of shape `(n_samples,)` with data labels corresponding to spiking activity.          |                | \n",
    "| n_labels   | `int`                      | The number of target labels in the data.                                                      |                | \n",
    "| rates      | `Optional[torch.Tensor]` | If passed, these represent spike rates from a previous `assign_labels()` call.              | None          | \n",
    "| alpha      | `float`                    | Rate of decay of label assignments.                                                           | 1             | \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.2342, 16.6958, 15.2342, 11.2342, 12.3151, 11.2342, 11.2342, 15.6958,\n",
      "        13.2342])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([15.7664, 13.0478, 13.7664, 15.7664, 11.3982, 12.7664, 14.7664, 14.0478,\n",
      "        15.7664])\n",
      "Neuron 1 Weights:\n",
      " tensor([12.6935, 17.2670, 15.6934, 11.6935, 12.8712, 11.6935, 11.6935, 16.2669,\n",
      "        13.6935])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([16.3257, 13.6190, 14.3257, 16.3257, 11.8574, 13.3257, 15.3257, 14.6190,\n",
      "        16.3257])\n",
      "Neuron 1 Weights:\n",
      " tensor([13.1628, 17.8482, 16.1627, 12.1628, 13.4342, 12.1628, 12.1628, 16.8481,\n",
      "        14.1627])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([16.8922, 14.2001, 14.8921, 16.8922, 12.3267, 13.8921, 15.8921, 15.2001,\n",
      "        16.8922])\n",
      "Neuron 1 Weights:\n",
      " tensor([13.6220, 18.4194, 16.6220, 12.6221, 13.9889, 12.6221, 12.6221, 17.4193,\n",
      "        14.6220])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([17.4499, 14.7713, 15.4498, 17.4499, 12.7860, 14.4498, 16.4498, 15.7713,\n",
      "        17.4499])\n",
      "Neuron 1 Weights:\n",
      " tensor([14.0813, 18.9905, 17.0813, 13.0813, 14.5450, 13.0813, 13.0813, 17.9905,\n",
      "        15.0813])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([18.0092, 15.3424, 16.0091, 18.0092, 13.2453, 15.0090, 17.0091, 16.3424,\n",
      "        18.0092])\n",
      "Neuron 1 Weights:\n",
      " tensor([14.5506, 19.5717, 17.5506, 13.5506, 15.1080, 13.5506, 13.5506, 18.5717,\n",
      "        15.5506])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([18.5756, 15.9235, 16.5755, 18.5756, 13.7145, 15.5755, 17.5756, 16.9236,\n",
      "        18.5756])\n",
      "Neuron 1 Weights:\n",
      " tensor([15.0099, 20.1429, 18.0099, 14.0099, 15.6627, 14.0099, 14.0099, 19.1428,\n",
      "        16.0099])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([19.1333, 16.4947, 17.1332, 19.1333, 14.1738, 16.1332, 18.1333, 17.4948,\n",
      "        19.1333])\n",
      "Neuron 1 Weights:\n",
      " tensor([15.4692, 20.7141, 18.4692, 14.4692, 16.2188, 14.4692, 14.4692, 19.7140,\n",
      "        16.4692])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([19.6926, 17.0659, 17.6925, 19.6926, 14.6331, 16.6924, 18.6925, 18.0660,\n",
      "        19.6926])\n",
      "Neuron 1 Weights:\n",
      " tensor([15.9384, 21.2952, 18.9385, 14.9384, 16.7818, 14.9384, 14.9384, 20.2952,\n",
      "        16.9385])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([20.2591, 17.6471, 18.2590, 20.2591, 15.1024, 17.2589, 19.2590, 18.6471,\n",
      "        20.2591])\n",
      "Neuron 1 Weights:\n",
      " tensor([16.3977, 21.8664, 19.3979, 15.3977, 17.3365, 15.3977, 15.3977, 20.8664,\n",
      "        17.3978])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([20.8167, 18.2182, 18.8166, 20.8167, 15.5617, 17.8166, 19.8167, 19.2183,\n",
      "        20.8167])\n",
      "Neuron 1 Weights:\n",
      " tensor([16.8570, 22.4376, 19.8572, 15.8570, 17.8927, 15.8570, 15.8570, 21.4375,\n",
      "        17.8571])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([21.3760, 18.7894, 19.3759, 21.3760, 16.0210, 18.3759, 20.3760, 19.7895,\n",
      "        21.3760])\n",
      "Neuron 1 Weights:\n",
      " tensor([17.3263, 23.0188, 20.3265, 16.3263, 18.4557, 16.3263, 16.3263, 22.0187,\n",
      "        18.3264])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([21.9425, 19.3706, 19.9424, 21.9425, 16.4902, 18.9424, 20.9424, 20.3707,\n",
      "        21.9425])\n",
      "Neuron 1 Weights:\n",
      " tensor([17.7856, 23.5900, 20.7858, 16.7856, 19.0103, 16.7856, 16.7856, 22.5899,\n",
      "        18.7857])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([22.5002, 19.9418, 20.5001, 22.5002, 16.9496, 19.5000, 21.5001, 20.9418,\n",
      "        22.5002])\n",
      "Neuron 1 Weights:\n",
      " tensor([18.2449, 24.1611, 21.2451, 17.2449, 19.5665, 17.2449, 17.2449, 23.1611,\n",
      "        19.2450])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([23.0595, 20.5130, 21.0594, 23.0595, 17.4089, 20.0593, 22.0594, 21.5130,\n",
      "        23.0595])\n",
      "Neuron 1 Weights:\n",
      " tensor([18.7142, 24.7423, 21.7144, 17.7142, 20.1295, 17.7142, 17.7142, 23.7422,\n",
      "        19.7143])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([23.6259, 21.0941, 21.6258, 23.6259, 17.8782, 20.6258, 22.6259, 22.0942,\n",
      "        23.6259])\n",
      "Neuron 1 Weights:\n",
      " tensor([19.1736, 25.3135, 22.1737, 18.1735, 20.6842, 18.1735, 18.1735, 24.3134,\n",
      "        20.1736])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([24.1836, 21.6653, 22.1835, 24.1836, 18.3375, 21.1835, 23.1836, 22.6654,\n",
      "        24.1836])\n",
      "Neuron 1 Weights:\n",
      " tensor([19.6329, 25.8846, 22.6330, 18.6328, 21.2403, 18.6328, 18.6328, 24.8846,\n",
      "        20.6329])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([24.7429, 22.2365, 22.7428, 24.7429, 18.7968, 21.7428, 23.7428, 23.2366,\n",
      "        24.7429])\n",
      "Neuron 1 Weights:\n",
      " tensor([20.1022, 26.4658, 23.1023, 19.1021, 21.8034, 19.1021, 19.1021, 25.4658,\n",
      "        21.1022])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([25.3094, 22.8176, 23.3093, 25.3094, 19.2661, 22.3092, 24.3093, 23.8177,\n",
      "        25.3094])\n",
      "Neuron 1 Weights:\n",
      " tensor([20.5615, 27.0370, 23.5616, 19.5614, 22.3580, 19.5614, 19.5614, 26.0369,\n",
      "        21.5615])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([25.8670, 23.3888, 23.8669, 25.8670, 19.7254, 22.8669, 24.8670, 24.3889,\n",
      "        25.8670])\n",
      "Neuron 1 Weights:\n",
      " tensor([21.0208, 27.6082, 24.0209, 20.0207, 22.9142, 20.0207, 20.0207, 26.6081,\n",
      "        22.0208])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([26.4263, 23.9600, 24.4262, 26.4263, 20.1847, 23.4262, 25.4263, 24.9601,\n",
      "        26.4263])\n",
      "Neuron 1 Weights:\n",
      " tensor([21.4901, 28.1894, 24.4902, 20.4900, 23.4772, 20.4900, 20.4900, 27.1893,\n",
      "        22.4901])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([26.9928, 24.5412, 24.9927, 26.9928, 20.6540, 23.9927, 25.9928, 25.5412,\n",
      "        26.9928])\n",
      "Neuron 1 Weights:\n",
      " tensor([21.9494, 28.7605, 24.9495, 20.9493, 24.0319, 20.9493, 20.9493, 27.7605,\n",
      "        22.9494])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([27.5505, 25.1124, 25.5504, 27.5505, 21.1133, 24.5503, 26.5504, 26.1124,\n",
      "        27.5505])\n",
      "Neuron 1 Weights:\n",
      " tensor([22.4087, 29.3317, 25.4088, 21.4086, 24.5880, 21.4086, 21.4086, 28.3316,\n",
      "        23.4087])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([28.1098, 25.6835, 26.1097, 28.1098, 21.5726, 25.1096, 27.1097, 26.6836,\n",
      "        28.1098])\n",
      "Neuron 1 Weights:\n",
      " tensor([22.8780, 29.9129, 25.8781, 21.8779, 25.1510, 21.8779, 21.8779, 28.9128,\n",
      "        23.8780])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([28.6762, 26.2647, 26.6761, 28.6762, 22.0419, 25.6761, 27.6762, 27.2648,\n",
      "        28.6762])\n",
      "Neuron 1 Weights:\n",
      " tensor([23.3373, 30.4841, 26.3374, 22.3373, 25.7057, 22.3373, 22.3373, 29.4840,\n",
      "        24.3373])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([29.2339, 26.8359, 27.2338, 29.2339, 22.5012, 26.2338, 28.2339, 27.8359,\n",
      "        29.2339])\n",
      "Neuron 1 Weights:\n",
      " tensor([23.7966, 31.0552, 26.7967, 22.7965, 26.2619, 22.7965, 22.7965, 30.0552,\n",
      "        24.7966])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([29.7932, 27.4071, 27.7931, 29.7932, 22.9605, 26.7931, 28.7932, 28.4071,\n",
      "        29.7932])\n",
      "Neuron 1 Weights:\n",
      " tensor([24.2659, 31.6364, 27.2660, 23.2659, 26.8249, 23.2659, 23.2659, 30.6364,\n",
      "        25.2659])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([30.3597, 27.9882, 28.3596, 30.3597, 23.4298, 27.3595, 29.3596, 28.9883,\n",
      "        30.3597])\n",
      "Neuron 1 Weights:\n",
      " tensor([24.7252, 32.2076, 27.7253, 23.7252, 27.3795, 23.7252, 23.7252, 31.2075,\n",
      "        25.7252])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([30.9173, 28.5594, 28.9173, 30.9173, 23.8891, 27.9172, 29.9173, 29.5595,\n",
      "        30.9173])\n",
      "Neuron 1 Weights:\n",
      " tensor([25.1845, 32.7786, 28.1846, 24.1845, 27.9357, 24.1845, 24.1845, 31.7787,\n",
      "        26.1845])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([31.4766, 29.1306, 29.4765, 31.4766, 24.3484, 28.4765, 30.4766, 30.1307,\n",
      "        31.4766])\n",
      "Neuron 1 Weights:\n",
      " tensor([25.6538, 33.3597, 28.6539, 24.6538, 28.4987, 24.6538, 24.6538, 32.3598,\n",
      "        26.6538])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([32.0431, 29.7118, 30.0430, 32.0431, 24.8177, 29.0430, 31.0431, 30.7118,\n",
      "        32.0431])\n",
      "Neuron 1 Weights:\n",
      " tensor([26.1131, 33.9307, 29.1132, 25.1131, 29.0534, 25.1131, 25.1131, 32.9309,\n",
      "        27.1132])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([32.6007, 30.2829, 30.6007, 32.6007, 25.2770, 29.6006, 31.6007, 31.2830,\n",
      "        32.6007])\n",
      "Neuron 1 Weights:\n",
      " tensor([26.5724, 34.5018, 29.5725, 25.5724, 29.6096, 25.5724, 25.5724, 33.5019,\n",
      "        27.5725])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([33.1599, 30.8541, 31.1600, 33.1599, 25.7363, 30.1599, 32.1600, 31.8542,\n",
      "        33.1599])\n",
      "Neuron 1 Weights:\n",
      " tensor([27.0417, 35.0829, 30.0418, 26.0417, 30.1726, 26.0417, 26.0417, 34.0830,\n",
      "        28.0418])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([33.7263, 31.4353, 31.7264, 33.7263, 26.2056, 30.7264, 32.7264, 32.4353,\n",
      "        33.7263])\n",
      "Neuron 1 Weights:\n",
      " tensor([27.5010, 35.6539, 30.5011, 26.5010, 30.7272, 26.5010, 26.5010, 34.6541,\n",
      "        28.5011])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([34.2838, 32.0065, 32.2841, 34.2838, 26.6649, 31.2841, 33.2840, 33.0063,\n",
      "        34.2838])\n",
      "Neuron 1 Weights:\n",
      " tensor([27.9603, 36.2250, 30.9604, 26.9603, 31.2834, 26.9603, 26.9603, 35.2251,\n",
      "        28.9604])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([34.8430, 32.5775, 32.8433, 34.8430, 27.1242, 31.8434, 33.8432, 33.5774,\n",
      "        34.8430])\n",
      "Neuron 1 Weights:\n",
      " tensor([28.4296, 36.8061, 31.4297, 27.4296, 31.8464, 27.4296, 27.4296, 35.8062,\n",
      "        29.4297])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([35.4094, 33.1586, 33.4097, 35.4094, 27.5935, 32.4098, 34.4095, 34.1585,\n",
      "        35.4094])\n",
      "Neuron 1 Weights:\n",
      " tensor([28.8889, 37.3771, 31.8891, 27.8889, 32.4010, 27.8889, 27.8889, 36.3773,\n",
      "        29.8890])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([35.9670, 33.7297, 33.9672, 35.9670, 28.0528, 32.9674, 34.9671, 34.7295,\n",
      "        35.9670])\n",
      "Neuron 1 Weights:\n",
      " tensor([29.3482, 37.9482, 32.3483, 28.3482, 32.9571, 28.3482, 28.3482, 36.9483,\n",
      "        30.3483])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([36.5262, 34.3007, 34.5264, 36.5262, 28.5122, 33.5266, 35.5263, 35.3006,\n",
      "        36.5262])\n",
      "Neuron 1 Weights:\n",
      " tensor([29.8175, 38.5293, 32.8176, 28.8175, 33.5200, 28.8175, 28.8175, 37.5294,\n",
      "        30.8176])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([37.0926, 34.8818, 35.0928, 37.0926, 28.9814, 34.0929, 36.0927, 35.8817,\n",
      "        37.0926])\n",
      "Neuron 1 Weights:\n",
      " tensor([30.2768, 39.1003, 33.2769, 29.2768, 34.0745, 29.2768, 29.2768, 38.1005,\n",
      "        31.2769])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([37.6502, 35.4529, 35.6504, 37.6502, 29.4408, 34.6505, 36.6503, 36.4527,\n",
      "        37.6502])\n",
      "Neuron 1 Weights:\n",
      " tensor([30.7361, 39.6714, 33.7362, 29.7361, 34.6306, 29.7361, 29.7361, 38.6715,\n",
      "        31.7362])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([38.2094, 36.0239, 36.2096, 38.2094, 29.9001, 35.2097, 37.2095, 37.0238,\n",
      "        38.2094])\n",
      "Neuron 1 Weights:\n",
      " tensor([31.2054, 40.2525, 34.2055, 30.2054, 35.1935, 30.2054, 30.2054, 39.2526,\n",
      "        32.2055])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([38.7757, 36.6050, 36.7760, 38.7757, 30.3694, 35.7761, 37.7759, 37.6048,\n",
      "        38.7757])\n",
      "Neuron 1 Weights:\n",
      " tensor([31.6648, 40.8235, 34.6648, 30.6647, 35.7481, 30.6647, 30.6647, 39.8237,\n",
      "        32.6648])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([39.3333, 37.1760, 37.3336, 39.3333, 30.8287, 36.3337, 38.3334, 38.1759,\n",
      "        39.3333])\n",
      "Neuron 1 Weights:\n",
      " tensor([32.1240, 41.3946, 35.1241, 31.1240, 36.3042, 31.1240, 31.1240, 40.3947,\n",
      "        33.1241])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([39.8925, 37.7471, 37.8928, 39.8925, 31.2880, 36.8929, 38.8926, 38.7470,\n",
      "        39.8925])\n",
      "Neuron 1 Weights:\n",
      " tensor([32.5933, 41.9757, 35.5934, 31.5933, 36.8671, 31.5933, 31.5933, 40.9758,\n",
      "        33.5933])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([40.4589, 38.3282, 38.4591, 40.4589, 31.7573, 37.4592, 39.4590, 39.3280,\n",
      "        40.4589])\n",
      "Neuron 1 Weights:\n",
      " tensor([33.0526, 42.5467, 36.0527, 32.0526, 37.4216, 32.0526, 32.0526, 41.5468,\n",
      "        34.0526])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([41.0165, 38.8992, 39.0167, 41.0165, 32.2166, 38.0168, 40.0166, 39.8991,\n",
      "        41.0165])\n",
      "Neuron 1 Weights:\n",
      " tensor([33.5119, 43.1178, 36.5119, 32.5119, 37.9777, 32.5119, 32.5119, 42.1179,\n",
      "        34.5119])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([41.5757, 39.4703, 39.5759, 41.5757, 32.6759, 38.5760, 40.5758, 40.4702,\n",
      "        41.5757])\n",
      "Neuron 1 Weights:\n",
      " tensor([33.9812, 43.6988, 36.9812, 32.9812, 38.5406, 32.9812, 32.9812, 42.6990,\n",
      "        34.9812])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([42.1421, 40.0514, 40.1423, 42.1421, 33.1451, 39.1424, 41.1422, 41.0512,\n",
      "        42.1421])\n",
      "Neuron 1 Weights:\n",
      " tensor([34.4405, 44.2699, 37.4405, 33.4405, 39.0952, 33.4405, 33.4405, 43.2700,\n",
      "        35.4405])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([42.6996, 40.6224, 40.6999, 42.6996, 33.6044, 39.7000, 41.6998, 41.6223,\n",
      "        42.6996])\n",
      "Neuron 1 Weights:\n",
      " tensor([34.8998, 44.8410, 37.8998, 33.8998, 39.6512, 33.8998, 33.8998, 43.8411,\n",
      "        35.8998])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([43.2588, 41.1935, 41.2591, 43.2588, 34.0637, 40.2592, 42.2590, 42.1934,\n",
      "        43.2588])\n",
      "Neuron 1 Weights:\n",
      " tensor([35.3691, 45.4220, 38.3691, 34.3691, 40.2142, 34.3691, 34.3691, 44.4222,\n",
      "        36.3691])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([43.8252, 41.7746, 41.8254, 43.8252, 34.5330, 40.8256, 42.8253, 42.7744,\n",
      "        43.8252])\n",
      "Neuron 1 Weights:\n",
      " tensor([35.8284, 45.9931, 38.8284, 34.8284, 40.7687, 34.8284, 34.8284, 44.9932,\n",
      "        36.8284])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([44.3828, 42.3456, 42.3830, 44.3828, 34.9923, 41.3832, 43.3829, 43.3455,\n",
      "        44.3828])\n",
      "Neuron 1 Weights:\n",
      " tensor([36.2876, 46.5642, 39.2877, 35.2876, 41.3248, 35.2876, 35.2876, 45.5643,\n",
      "        37.2876])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([44.9420, 42.9167, 42.9422, 44.9420, 35.4516, 41.9424, 43.9421, 43.9166,\n",
      "        44.9420])\n",
      "Neuron 1 Weights:\n",
      " tensor([36.7569, 47.1452, 39.7569, 35.7569, 41.8877, 35.7569, 35.7569, 46.1454,\n",
      "        37.7569])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([45.5084, 43.4978, 43.5086, 45.5084, 35.9209, 42.5087, 44.5085, 44.4976,\n",
      "        45.5084])\n",
      "Neuron 1 Weights:\n",
      " tensor([37.2162, 47.7163, 40.2162, 36.2162, 42.4423, 36.2162, 36.2162, 46.7164,\n",
      "        38.2162])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([46.0660, 44.0688, 44.0662, 46.0660, 36.3802, 43.0663, 45.0661, 45.0687,\n",
      "        46.0660])\n",
      "Neuron 1 Weights:\n",
      " tensor([37.6755, 48.2874, 40.6755, 36.6755, 42.9983, 36.6755, 36.6755, 47.2875,\n",
      "        38.6755])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([46.6252, 44.6399, 44.6254, 46.6252, 36.8395, 43.6255, 45.6253, 45.6398,\n",
      "        46.6252])\n",
      "Neuron 1 Weights:\n",
      " tensor([38.1448, 48.8684, 41.1448, 37.1448, 43.5613, 37.1448, 37.1448, 47.8686,\n",
      "        39.1448])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([47.1915, 45.2209, 45.1918, 47.1915, 37.3087, 44.1919, 46.1917, 46.2208,\n",
      "        47.1915])\n",
      "Neuron 1 Weights:\n",
      " tensor([38.6041, 49.4395, 41.6041, 37.6041, 44.1158, 37.6041, 37.6041, 48.4396,\n",
      "        39.6041])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([47.7491, 45.7920, 45.7494, 47.7491, 37.7680, 44.7495, 46.7492, 46.7919,\n",
      "        47.7491])\n",
      "Neuron 1 Weights:\n",
      " tensor([39.0634, 50.0105, 42.0634, 38.0634, 44.6719, 38.0634, 38.0634, 49.0107,\n",
      "        40.0634])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([48.3083, 46.3631, 46.3086, 48.3083, 38.2273, 45.3087, 47.3084, 47.3630,\n",
      "        48.3083])\n",
      "Neuron 1 Weights:\n",
      " tensor([39.5326, 50.5916, 42.5327, 38.5326, 45.2348, 38.5326, 38.5326, 49.5918,\n",
      "        40.5327])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([48.8747, 46.9441, 46.8749, 48.8747, 38.6966, 45.8750, 47.8748, 47.9440,\n",
      "        48.8747])\n",
      "Neuron 1 Weights:\n",
      " tensor([39.9919, 51.1627, 42.9920, 38.9919, 45.7894, 38.9919, 38.9919, 50.1628,\n",
      "        40.9920])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([49.4323, 47.5152, 47.4325, 49.4323, 39.1559, 46.4326, 48.4324, 48.5151,\n",
      "        49.4323])\n",
      "Neuron 1 Weights:\n",
      " tensor([40.4512, 51.7337, 43.4512, 39.4512, 46.3454, 39.4512, 39.4512, 50.7339,\n",
      "        41.4512])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([49.9915, 48.0863, 47.9917, 49.9915, 39.6152, 46.9918, 48.9916, 49.0862,\n",
      "        49.9915])\n",
      "Neuron 1 Weights:\n",
      " tensor([40.9205, 52.3148, 43.9205, 39.9205, 46.9084, 39.9205, 39.9205, 51.3149,\n",
      "        41.9205])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([50.5578, 48.6673, 48.5581, 50.5578, 40.0844, 47.5582, 49.5580, 49.6672,\n",
      "        50.5578])\n",
      "Neuron 1 Weights:\n",
      " tensor([41.3798, 52.8859, 44.3798, 40.3798, 47.4629, 40.3798, 40.3798, 51.8860,\n",
      "        42.3798])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([51.1154, 49.2384, 49.1157, 51.1154, 40.5438, 48.1158, 50.1156, 50.2383,\n",
      "        51.1154])\n",
      "Neuron 1 Weights:\n",
      " tensor([41.8391, 53.4569, 44.8391, 40.8391, 48.0190, 40.8391, 40.8391, 52.4571,\n",
      "        42.8391])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([51.6746, 49.8095, 49.6749, 51.6746, 41.0030, 48.6750, 50.6748, 50.8094,\n",
      "        51.6746])\n",
      "Neuron 1 Weights:\n",
      " tensor([42.3084, 54.0380, 45.3084, 41.3084, 48.5819, 41.3084, 41.3084, 53.0381,\n",
      "        43.3084])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([52.2410, 50.3905, 50.2412, 52.2410, 41.4723, 49.2414, 51.2411, 51.3904,\n",
      "        52.2410])\n",
      "Neuron 1 Weights:\n",
      " tensor([42.7677, 54.6091, 45.7677, 41.7677, 49.1365, 41.7677, 41.7677, 53.6092,\n",
      "        43.7677])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([52.7986, 50.9616, 50.7988, 52.7986, 41.9316, 49.7989, 51.7987, 51.9615,\n",
      "        52.7986])\n",
      "Neuron 1 Weights:\n",
      " tensor([43.2270, 55.1801, 46.2270, 42.2269, 49.6925, 42.2269, 42.2269, 54.1803,\n",
      "        44.2270])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([53.3578, 51.5327, 51.3580, 53.3578, 42.3909, 50.3581, 52.3579, 52.5326,\n",
      "        53.3578])\n",
      "Neuron 1 Weights:\n",
      " tensor([43.6962, 55.7612, 46.6963, 42.6962, 50.2554, 42.6962, 42.6962, 54.7613,\n",
      "        44.6963])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([53.9242, 52.1137, 51.9244, 53.9242, 42.8602, 50.9245, 52.9243, 53.1136,\n",
      "        53.9242])\n",
      "Neuron 1 Weights:\n",
      " tensor([44.1555, 56.3323, 47.1556, 43.1555, 50.8100, 43.1555, 43.1555, 55.3324,\n",
      "        45.1555])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([54.4818, 52.6848, 52.4820, 54.4818, 43.3195, 51.4821, 53.4819, 53.6847,\n",
      "        54.4818])\n",
      "Neuron 1 Weights:\n",
      " tensor([44.6148, 56.9033, 47.6148, 43.6148, 51.3661, 43.6148, 43.6148, 55.9035,\n",
      "        45.6148])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([55.0410, 53.2559, 53.0412, 55.0410, 43.7788, 52.0413, 54.0411, 54.2557,\n",
      "        55.0410])\n",
      "Neuron 1 Weights:\n",
      " tensor([45.0841, 57.4844, 48.0841, 44.0841, 51.9290, 44.0841, 44.0841, 56.4845,\n",
      "        46.0841])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([55.6073, 53.8369, 53.6076, 55.6073, 44.2480, 52.6077, 54.6074, 54.8368,\n",
      "        55.6073])\n",
      "Neuron 1 Weights:\n",
      " tensor([45.5434, 58.0555, 48.5434, 44.5434, 52.4835, 44.5434, 44.5434, 57.0556,\n",
      "        46.5434])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([56.1649, 54.4080, 54.1651, 56.1649, 44.7073, 53.1653, 55.1650, 55.4079,\n",
      "        56.1649])\n",
      "Neuron 1 Weights:\n",
      " tensor([46.0027, 58.6265, 49.0027, 45.0027, 53.0396, 45.0027, 45.0027, 57.6267,\n",
      "        47.0027])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([56.7241, 54.9791, 54.7243, 56.7241, 45.1666, 53.7245, 55.7242, 55.9789,\n",
      "        56.7241])\n",
      "Neuron 1 Weights:\n",
      " tensor([46.4720, 59.2076, 49.4720, 45.4720, 53.6025, 45.4720, 45.4720, 58.2077,\n",
      "        47.4720])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([57.2905, 55.5601, 55.2907, 57.2905, 45.6359, 54.2908, 56.2906, 56.5600,\n",
      "        57.2905])\n",
      "Neuron 1 Weights:\n",
      " tensor([46.9313, 59.7787, 49.9313, 45.9313, 54.1571, 45.9313, 45.9313, 58.7788,\n",
      "        47.9313])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([57.8481, 56.1312, 55.8483, 57.8481, 46.0952, 54.8484, 56.8482, 57.1311,\n",
      "        57.8481])\n",
      "Neuron 1 Weights:\n",
      " tensor([47.3905, 60.3497, 50.3906, 46.3905, 54.7132, 46.3905, 46.3905, 59.3498,\n",
      "        48.3906])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([58.4073, 56.7023, 56.4075, 58.4073, 46.5545, 55.4076, 57.4074, 57.7021,\n",
      "        58.4073])\n",
      "Neuron 1 Weights:\n",
      " tensor([47.8598, 60.9308, 50.8598, 46.8598, 55.2761, 46.8598, 46.8598, 59.9309,\n",
      "        48.8598])\n",
      "====================\n",
      "Neuron 0 Weights:\n",
      " tensor([58.9736, 57.2833, 56.9739, 58.9736, 47.0238, 55.9740, 57.9738, 58.2832,\n",
      "        58.9736])\n",
      "Neuron 1 Weights:\n",
      " tensor([48.3191, 61.5019, 51.3191, 47.3191, 55.8306, 47.3191, 47.3191, 60.5020,\n",
      "        49.3191])\n",
      "====================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 368.925 248.518125\" width=\"368.925pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-11-19T16:34:31.080952</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 248.518125 \nL 368.925 248.518125 \nL 368.925 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 361.725 224.64 \nL 361.725 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mb7f1bb71ac\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.143182\" xlink:href=\"#mb7f1bb71ac\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(38.961932 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"103.630785\" xlink:href=\"#mb7f1bb71ac\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(97.268285 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"165.118388\" xlink:href=\"#mb7f1bb71ac\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(158.755888 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"226.605992\" xlink:href=\"#mb7f1bb71ac\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(220.243492 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"288.093595\" xlink:href=\"#mb7f1bb71ac\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(281.731095 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.581198\" xlink:href=\"#mb7f1bb71ac\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(340.037448 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5b95539108\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5b95539108\" y=\"216.555968\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 220.355186)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5b95539108\" y=\"182.731989\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 186.531207)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5b95539108\" y=\"148.908009\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 152.707228)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5b95539108\" y=\"115.08403\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 30 -->\n      <g transform=\"translate(7.2 118.883249)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5b95539108\" y=\"81.260051\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(7.2 85.05927)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5b95539108\" y=\"47.436072\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(7.2 51.235291)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5b95539108\" y=\"13.612093\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(7.2 17.411312)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#pa8c7e465a9)\" d=\"M 42.143182 204.609172 \nL 45.217562 202.757275 \nL 48.291942 200.810906 \nL 51.366322 199.006772 \nL 54.440702 197.198893 \nL 57.515083 195.391014 \nL 60.589463 193.583135 \nL 63.663843 191.696906 \nL 66.738223 189.805212 \nL 69.812603 187.889254 \nL 72.886983 186.003029 \nL 75.961364 184.111355 \nL 79.035744 182.195399 \nL 82.110124 180.309174 \nL 85.184504 178.4175 \nL 88.258884 176.501544 \nL 91.333264 174.615319 \nL 94.407645 172.723645 \nL 97.482025 170.807689 \nL 100.556405 168.921464 \nL 103.630785 167.02979 \nL 106.705165 165.113834 \nL 109.779545 163.227609 \nL 112.853926 161.3359 \nL 115.928306 159.419876 \nL 119.002686 157.533586 \nL 122.077066 155.641851 \nL 125.151446 153.725827 \nL 128.225826 151.839537 \nL 131.300207 149.947803 \nL 134.374587 148.031779 \nL 137.448967 146.145489 \nL 140.523347 144.253754 \nL 143.597727 142.33773 \nL 146.672107 140.45144 \nL 149.746488 138.559706 \nL 152.820868 136.643682 \nL 155.895248 134.757392 \nL 158.969628 132.865657 \nL 162.044008 130.949633 \nL 165.118388 129.063343 \nL 168.192769 127.171609 \nL 171.267149 125.255585 \nL 174.341529 123.369295 \nL 177.415909 121.47756 \nL 180.490289 119.561536 \nL 183.564669 117.675246 \nL 186.63905 115.783512 \nL 189.71343 113.867487 \nL 192.78781 111.981198 \nL 195.86219 110.089463 \nL 198.93657 108.173445 \nL 202.01095 106.287465 \nL 205.085331 104.396028 \nL 208.159711 102.480332 \nL 211.234091 100.594352 \nL 214.308471 98.702914 \nL 217.382851 96.787219 \nL 220.457231 94.901239 \nL 223.531612 93.009801 \nL 226.605992 91.094106 \nL 229.680372 89.208126 \nL 232.754752 87.316688 \nL 235.829132 85.400993 \nL 238.903512 83.515013 \nL 241.977893 81.623575 \nL 245.052273 79.70788 \nL 248.126653 77.8219 \nL 251.201033 75.930462 \nL 254.275413 74.014767 \nL 257.349793 72.128787 \nL 260.424174 70.237349 \nL 263.498554 68.321654 \nL 266.572934 66.435674 \nL 269.647314 64.544236 \nL 272.721694 62.628541 \nL 275.796074 60.742561 \nL 278.870455 58.851123 \nL 281.944835 56.935428 \nL 285.019215 55.049448 \nL 288.093595 53.15801 \nL 291.167975 51.242315 \nL 294.242355 49.356335 \nL 297.316736 47.464897 \nL 300.391116 45.549202 \nL 303.465496 43.663222 \nL 306.539876 41.771784 \nL 309.614256 39.856089 \nL 312.688636 37.970109 \nL 315.763017 36.078671 \nL 318.837397 34.162976 \nL 321.911777 32.276996 \nL 324.986157 30.385558 \nL 328.060537 28.469862 \nL 331.134917 26.583883 \nL 334.209298 24.692445 \nL 337.283678 22.776749 \nL 340.358058 20.890769 \nL 343.432438 18.999332 \nL 346.506818 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pa8c7e465a9)\" d=\"M 42.143182 214.7238 \nL 45.217562 212.825051 \nL 48.291942 210.825088 \nL 51.366322 208.994319 \nL 54.440702 207.163473 \nL 57.515083 205.332627 \nL 60.589463 203.501781 \nL 63.663843 201.569888 \nL 66.738223 199.637964 \nL 69.812603 197.672335 \nL 72.886983 195.740472 \nL 75.961364 193.808549 \nL 79.035744 191.842919 \nL 82.110124 189.911057 \nL 85.184504 187.979157 \nL 88.258884 186.013554 \nL 91.333264 184.081721 \nL 94.407645 182.149823 \nL 97.482025 180.184219 \nL 100.556405 178.252386 \nL 103.630785 176.320488 \nL 106.705165 174.354885 \nL 109.779545 172.423051 \nL 112.853926 170.491154 \nL 115.928306 168.52555 \nL 119.002686 166.593717 \nL 122.077066 164.661819 \nL 125.151446 162.696216 \nL 128.225826 160.764285 \nL 131.300207 158.832281 \nL 134.374587 156.866568 \nL 137.448967 154.934628 \nL 140.523347 153.002624 \nL 143.597727 151.036911 \nL 146.672107 149.104971 \nL 149.746488 147.172967 \nL 152.820868 145.207254 \nL 155.895248 143.275314 \nL 158.969628 141.343309 \nL 162.044008 139.377596 \nL 165.118388 137.445657 \nL 168.192769 135.513652 \nL 171.267149 133.547939 \nL 174.341529 131.615999 \nL 177.415909 129.683995 \nL 180.490289 127.718282 \nL 183.564669 125.786342 \nL 186.63905 123.854338 \nL 189.71343 121.888625 \nL 192.78781 119.956685 \nL 195.86219 118.024681 \nL 198.93657 116.058968 \nL 202.01095 114.127028 \nL 205.085331 112.195023 \nL 208.159711 110.22931 \nL 211.234091 108.297377 \nL 214.308471 106.365747 \nL 217.382851 104.400427 \nL 220.457231 102.468849 \nL 223.531612 100.537219 \nL 226.605992 98.571899 \nL 229.680372 96.640321 \nL 232.754752 94.70869 \nL 235.829132 92.743371 \nL 238.903512 90.811792 \nL 241.977893 88.880162 \nL 245.052273 86.914843 \nL 248.126653 84.983264 \nL 251.201033 83.051634 \nL 254.275413 81.086315 \nL 257.349793 79.154736 \nL 260.424174 77.223106 \nL 263.498554 75.257786 \nL 266.572934 73.326208 \nL 269.647314 71.394578 \nL 272.721694 69.429258 \nL 275.796074 67.49768 \nL 278.870455 65.566049 \nL 281.944835 63.60073 \nL 285.019215 61.669151 \nL 288.093595 59.737521 \nL 291.167975 57.772202 \nL 294.242355 55.840623 \nL 297.316736 53.908993 \nL 300.391116 51.943674 \nL 303.465496 50.012095 \nL 306.539876 48.080465 \nL 309.614256 46.115145 \nL 312.688636 44.183567 \nL 315.763017 42.251937 \nL 318.837397 40.286617 \nL 321.911777 38.355039 \nL 324.986157 36.423408 \nL 328.060537 34.458089 \nL 331.134917 32.52651 \nL 334.209298 30.59488 \nL 337.283678 28.629561 \nL 340.358058 26.697982 \nL 343.432438 24.766352 \nL 346.506818 22.801033 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#pa8c7e465a9)\" d=\"M 42.143182 211.373968 \nL 45.217562 209.522074 \nL 48.291942 207.575742 \nL 51.366322 205.771647 \nL 54.440702 203.963812 \nL 57.515083 202.155956 \nL 60.589463 200.348077 \nL 63.663843 198.461849 \nL 66.738223 196.570154 \nL 69.812603 194.654182 \nL 72.886983 192.767937 \nL 75.961364 190.876243 \nL 79.035744 188.960272 \nL 82.110124 187.074047 \nL 85.184504 185.182374 \nL 88.258884 183.266417 \nL 91.333264 181.380192 \nL 94.407645 179.488519 \nL 97.482025 177.572562 \nL 100.556405 175.686337 \nL 103.630785 173.794664 \nL 106.705165 171.878707 \nL 109.779545 169.992482 \nL 112.853926 168.100809 \nL 115.928306 166.184852 \nL 119.002686 164.298627 \nL 122.077066 162.406957 \nL 125.151446 160.490933 \nL 128.225826 158.604643 \nL 131.300207 156.712908 \nL 134.374587 154.796884 \nL 137.448967 152.910594 \nL 140.523347 151.01886 \nL 143.597727 149.102836 \nL 146.672107 147.216546 \nL 149.746488 145.324811 \nL 152.820868 143.408787 \nL 155.895248 141.522497 \nL 158.969628 139.630763 \nL 162.044008 137.714739 \nL 165.118388 135.828449 \nL 168.192769 133.936714 \nL 171.267149 132.02069 \nL 174.341529 130.1344 \nL 177.415909 128.242666 \nL 180.490289 126.326642 \nL 183.564669 124.440352 \nL 186.63905 122.548617 \nL 189.71343 120.632593 \nL 192.78781 118.746303 \nL 195.86219 116.854569 \nL 198.93657 114.938544 \nL 202.01095 113.052255 \nL 205.085331 111.16052 \nL 208.159711 109.244496 \nL 211.234091 107.358361 \nL 214.308471 105.466923 \nL 217.382851 103.551228 \nL 220.457231 101.665248 \nL 223.531612 99.77381 \nL 226.605992 97.858115 \nL 229.680372 95.972135 \nL 232.754752 94.080697 \nL 235.829132 92.165002 \nL 238.903512 90.279022 \nL 241.977893 88.387584 \nL 245.052273 86.471889 \nL 248.126653 84.585909 \nL 251.201033 82.694471 \nL 254.275413 80.778776 \nL 257.349793 78.892796 \nL 260.424174 77.001358 \nL 263.498554 75.085663 \nL 266.572934 73.199683 \nL 269.647314 71.308245 \nL 272.721694 69.39255 \nL 275.796074 67.50657 \nL 278.870455 65.615132 \nL 281.944835 63.699437 \nL 285.019215 61.813457 \nL 288.093595 59.922019 \nL 291.167975 58.006324 \nL 294.242355 56.120344 \nL 297.316736 54.228906 \nL 300.391116 52.31321 \nL 303.465496 50.42723 \nL 306.539876 48.535793 \nL 309.614256 46.620097 \nL 312.688636 44.734117 \nL 315.763017 42.84268 \nL 318.837397 40.926984 \nL 321.911777 39.041004 \nL 324.986157 37.149566 \nL 328.060537 35.233871 \nL 331.134917 33.347891 \nL 334.209298 31.456453 \nL 337.283678 29.540758 \nL 340.358058 27.654778 \nL 343.432438 25.76334 \nL 346.506818 23.847645 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#pa8c7e465a9)\" d=\"M 42.143182 204.609172 \nL 45.217562 202.757275 \nL 48.291942 200.810906 \nL 51.366322 199.006772 \nL 54.440702 197.198893 \nL 57.515083 195.391014 \nL 60.589463 193.583135 \nL 63.663843 191.696906 \nL 66.738223 189.805212 \nL 69.812603 187.889254 \nL 72.886983 186.003029 \nL 75.961364 184.111355 \nL 79.035744 182.195399 \nL 82.110124 180.309174 \nL 85.184504 178.4175 \nL 88.258884 176.501544 \nL 91.333264 174.615319 \nL 94.407645 172.723645 \nL 97.482025 170.807689 \nL 100.556405 168.921464 \nL 103.630785 167.02979 \nL 106.705165 165.113834 \nL 109.779545 163.227609 \nL 112.853926 161.3359 \nL 115.928306 159.419876 \nL 119.002686 157.533586 \nL 122.077066 155.641851 \nL 125.151446 153.725827 \nL 128.225826 151.839537 \nL 131.300207 149.947803 \nL 134.374587 148.031779 \nL 137.448967 146.145489 \nL 140.523347 144.253754 \nL 143.597727 142.33773 \nL 146.672107 140.45144 \nL 149.746488 138.559706 \nL 152.820868 136.643682 \nL 155.895248 134.757392 \nL 158.969628 132.865657 \nL 162.044008 130.949633 \nL 165.118388 129.063343 \nL 168.192769 127.171609 \nL 171.267149 125.255585 \nL 174.341529 123.369295 \nL 177.415909 121.47756 \nL 180.490289 119.561536 \nL 183.564669 117.675246 \nL 186.63905 115.783512 \nL 189.71343 113.867487 \nL 192.78781 111.981198 \nL 195.86219 110.089463 \nL 198.93657 108.173445 \nL 202.01095 106.287465 \nL 205.085331 104.396028 \nL 208.159711 102.480332 \nL 211.234091 100.594352 \nL 214.308471 98.702914 \nL 217.382851 96.787219 \nL 220.457231 94.901239 \nL 223.531612 93.009801 \nL 226.605992 91.094106 \nL 229.680372 89.208126 \nL 232.754752 87.316688 \nL 235.829132 85.400993 \nL 238.903512 83.515013 \nL 241.977893 81.623575 \nL 245.052273 79.70788 \nL 248.126653 77.8219 \nL 251.201033 75.930462 \nL 254.275413 74.014767 \nL 257.349793 72.128787 \nL 260.424174 70.237349 \nL 263.498554 68.321654 \nL 266.572934 66.435674 \nL 269.647314 64.544236 \nL 272.721694 62.628541 \nL 275.796074 60.742561 \nL 278.870455 58.851123 \nL 281.944835 56.935428 \nL 285.019215 55.049448 \nL 288.093595 53.15801 \nL 291.167975 51.242315 \nL 294.242355 49.356335 \nL 297.316736 47.464897 \nL 300.391116 45.549202 \nL 303.465496 43.663222 \nL 306.539876 41.771784 \nL 309.614256 39.856089 \nL 312.688636 37.970109 \nL 315.763017 36.078671 \nL 318.837397 34.162976 \nL 321.911777 32.276996 \nL 324.986157 30.385558 \nL 328.060537 28.469862 \nL 331.134917 26.583883 \nL 334.209298 24.692445 \nL 337.283678 22.776749 \nL 340.358058 20.890769 \nL 343.432438 18.999332 \nL 346.506818 17.083636 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#pa8c7e465a9)\" d=\"M 42.143182 211.979555 \nL 45.217562 210.459263 \nL 48.291942 208.837948 \nL 51.366322 207.385483 \nL 54.440702 205.932887 \nL 57.515083 204.480291 \nL 60.589463 203.027695 \nL 63.663843 201.474089 \nL 66.738223 199.920597 \nL 69.812603 198.333342 \nL 72.886983 196.779791 \nL 75.961364 195.2263 \nL 79.035744 193.639044 \nL 82.110124 192.085494 \nL 85.184504 190.532002 \nL 88.258884 188.944756 \nL 91.333264 187.391237 \nL 94.407645 185.837772 \nL 97.482025 184.250548 \nL 100.556405 182.697028 \nL 103.630785 181.143564 \nL 106.705165 179.556339 \nL 109.779545 178.00282 \nL 112.853926 176.449355 \nL 115.928306 174.86213 \nL 119.002686 173.308611 \nL 122.077066 171.755146 \nL 125.151446 170.167922 \nL 128.225826 168.614402 \nL 131.300207 167.060938 \nL 134.374587 165.473713 \nL 137.448967 163.920194 \nL 140.523347 162.366719 \nL 143.597727 160.779408 \nL 146.672107 159.225804 \nL 149.746488 157.672253 \nL 152.820868 156.084941 \nL 155.895248 154.531338 \nL 158.969628 152.977786 \nL 162.044008 151.390474 \nL 165.118388 149.836871 \nL 168.192769 148.283319 \nL 171.267149 146.696007 \nL 174.341529 145.142404 \nL 177.415909 143.588852 \nL 180.490289 142.001541 \nL 183.564669 140.447937 \nL 186.63905 138.894386 \nL 189.71343 137.307074 \nL 192.78781 135.753471 \nL 195.86219 134.199919 \nL 198.93657 132.612607 \nL 202.01095 131.059004 \nL 205.085331 129.505452 \nL 208.159711 127.91814 \nL 211.234091 126.364537 \nL 214.308471 124.810985 \nL 217.382851 123.223674 \nL 220.457231 121.67007 \nL 223.531612 120.116519 \nL 226.605992 118.529207 \nL 229.680372 116.975604 \nL 232.754752 115.422052 \nL 235.829132 113.83474 \nL 238.903512 112.281137 \nL 241.977893 110.727585 \nL 245.052273 109.140273 \nL 248.126653 107.586689 \nL 251.201033 106.033202 \nL 254.275413 104.445949 \nL 257.349793 102.892384 \nL 260.424174 101.338897 \nL 263.498554 99.751643 \nL 266.572934 98.198079 \nL 269.647314 96.644591 \nL 272.721694 95.057338 \nL 275.796074 93.503773 \nL 278.870455 91.950286 \nL 281.944835 90.363032 \nL 285.019215 88.809468 \nL 288.093595 87.255981 \nL 291.167975 85.668727 \nL 294.242355 84.115162 \nL 297.316736 82.561675 \nL 300.391116 80.974421 \nL 303.465496 79.420857 \nL 306.539876 77.86737 \nL 309.614256 76.280116 \nL 312.688636 74.726551 \nL 315.763017 73.173064 \nL 318.837397 71.58581 \nL 321.911777 70.032246 \nL 324.986157 68.478759 \nL 328.060537 66.891505 \nL 331.134917 65.33794 \nL 334.209298 63.784453 \nL 337.283678 62.197199 \nL 340.358058 60.643635 \nL 343.432438 59.090148 \nL 346.506818 57.502894 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#pa8c7e465a9)\" d=\"M 42.143182 214.756364 \nL 45.217562 212.904472 \nL 48.291942 210.958136 \nL 51.366322 209.15404 \nL 54.440702 207.346205 \nL 57.515083 205.53837 \nL 60.589463 203.730536 \nL 63.663843 201.844323 \nL 66.738223 199.952628 \nL 69.812603 198.036656 \nL 72.886983 196.150411 \nL 75.961364 194.258717 \nL 79.035744 192.342744 \nL 82.110124 190.4565 \nL 85.184504 188.564817 \nL 88.258884 186.64886 \nL 91.333264 184.762635 \nL 94.407645 182.870962 \nL 97.482025 180.955005 \nL 100.556405 179.06878 \nL 103.630785 177.177107 \nL 106.705165 175.26115 \nL 109.779545 173.374925 \nL 112.853926 171.483252 \nL 115.928306 169.567295 \nL 119.002686 167.68107 \nL 122.077066 165.789397 \nL 125.151446 163.87344 \nL 128.225826 161.987202 \nL 131.300207 160.095468 \nL 134.374587 158.179443 \nL 137.448967 156.293154 \nL 140.523347 154.401419 \nL 143.597727 152.485395 \nL 146.672107 150.599105 \nL 149.746488 148.70737 \nL 152.820868 146.791346 \nL 155.895248 144.905057 \nL 158.969628 143.013322 \nL 162.044008 141.097298 \nL 165.118388 139.211008 \nL 168.192769 137.319273 \nL 171.267149 135.403249 \nL 174.341529 133.51696 \nL 177.415909 131.625225 \nL 180.490289 129.709201 \nL 183.564669 127.822911 \nL 186.63905 125.931176 \nL 189.71343 124.015152 \nL 192.78781 122.128863 \nL 195.86219 120.237128 \nL 198.93657 118.321104 \nL 202.01095 116.434814 \nL 205.085331 114.543079 \nL 208.159711 112.627055 \nL 211.234091 110.740765 \nL 214.308471 108.849031 \nL 217.382851 106.933226 \nL 220.457231 105.047246 \nL 223.531612 103.155808 \nL 226.605992 101.240113 \nL 229.680372 99.354133 \nL 232.754752 97.462695 \nL 235.829132 95.547 \nL 238.903512 93.66102 \nL 241.977893 91.769582 \nL 245.052273 89.853887 \nL 248.126653 87.967907 \nL 251.201033 86.076469 \nL 254.275413 84.160774 \nL 257.349793 82.274794 \nL 260.424174 80.383356 \nL 263.498554 78.467661 \nL 266.572934 76.581681 \nL 269.647314 74.690243 \nL 272.721694 72.774548 \nL 275.796074 70.888568 \nL 278.870455 68.99713 \nL 281.944835 67.081435 \nL 285.019215 65.195455 \nL 288.093595 63.304017 \nL 291.167975 61.388321 \nL 294.242355 59.502341 \nL 297.316736 57.610904 \nL 300.391116 55.695208 \nL 303.465496 53.809228 \nL 306.539876 51.917791 \nL 309.614256 50.002095 \nL 312.688636 48.116115 \nL 315.763017 46.224677 \nL 318.837397 44.308982 \nL 321.911777 42.423002 \nL 324.986157 40.531564 \nL 328.060537 38.615869 \nL 331.134917 36.729889 \nL 334.209298 34.838451 \nL 337.283678 32.922756 \nL 340.358058 31.036776 \nL 343.432438 29.145338 \nL 346.506818 27.229643 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path clip-path=\"url(#pa8c7e465a9)\" d=\"M 42.143182 207.99157 \nL 45.217562 206.139677 \nL 48.291942 204.193346 \nL 51.366322 202.389236 \nL 54.440702 200.581357 \nL 57.515083 198.773478 \nL 60.589463 196.965599 \nL 63.663843 195.07937 \nL 66.738223 193.187676 \nL 69.812603 191.271703 \nL 72.886983 189.385459 \nL 75.961364 187.493785 \nL 79.035744 185.577829 \nL 82.110124 183.691604 \nL 85.184504 181.79993 \nL 88.258884 179.883974 \nL 91.333264 177.997749 \nL 94.407645 176.106075 \nL 97.482025 174.190119 \nL 100.556405 172.303894 \nL 103.630785 170.412221 \nL 106.705165 168.496264 \nL 109.779545 166.610039 \nL 112.853926 164.718366 \nL 115.928306 162.802409 \nL 119.002686 160.916132 \nL 122.077066 159.024398 \nL 125.151446 157.108373 \nL 128.225826 155.222084 \nL 131.300207 153.330349 \nL 134.374587 151.414325 \nL 137.448967 149.528035 \nL 140.523347 147.636301 \nL 143.597727 145.720276 \nL 146.672107 143.833987 \nL 149.746488 141.942252 \nL 152.820868 140.026228 \nL 155.895248 138.139938 \nL 158.969628 136.248204 \nL 162.044008 134.332179 \nL 165.118388 132.44589 \nL 168.192769 130.554155 \nL 171.267149 128.638131 \nL 174.341529 126.751841 \nL 177.415909 124.860107 \nL 180.490289 122.944082 \nL 183.564669 121.057793 \nL 186.63905 119.166058 \nL 189.71343 117.250034 \nL 192.78781 115.363744 \nL 195.86219 113.472009 \nL 198.93657 111.555985 \nL 202.01095 109.669696 \nL 205.085331 107.778025 \nL 208.159711 105.86233 \nL 211.234091 103.97635 \nL 214.308471 102.084912 \nL 217.382851 100.169217 \nL 220.457231 98.283237 \nL 223.531612 96.391799 \nL 226.605992 94.476104 \nL 229.680372 92.590124 \nL 232.754752 90.698686 \nL 235.829132 88.782991 \nL 238.903512 86.897011 \nL 241.977893 85.005573 \nL 245.052273 83.089878 \nL 248.126653 81.203898 \nL 251.201033 79.31246 \nL 254.275413 77.396765 \nL 257.349793 75.510785 \nL 260.424174 73.619347 \nL 263.498554 71.703652 \nL 266.572934 69.817672 \nL 269.647314 67.926234 \nL 272.721694 66.010539 \nL 275.796074 64.124559 \nL 278.870455 62.233121 \nL 281.944835 60.317426 \nL 285.019215 58.431446 \nL 288.093595 56.540008 \nL 291.167975 54.624313 \nL 294.242355 52.738333 \nL 297.316736 50.846895 \nL 300.391116 48.9312 \nL 303.465496 47.04522 \nL 306.539876 45.153782 \nL 309.614256 43.238087 \nL 312.688636 41.352107 \nL 315.763017 39.460669 \nL 318.837397 37.544973 \nL 321.911777 35.658993 \nL 324.986157 33.767556 \nL 328.060537 31.85186 \nL 331.134917 29.96588 \nL 334.209298 28.074443 \nL 337.283678 26.158747 \nL 340.358058 24.272767 \nL 343.432438 22.381329 \nL 346.506818 20.465634 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path clip-path=\"url(#pa8c7e465a9)\" d=\"M 42.143182 211.341402 \nL 45.217562 209.44265 \nL 48.291942 207.442691 \nL 51.366322 205.611924 \nL 54.440702 203.781078 \nL 57.515083 201.950208 \nL 60.589463 200.119321 \nL 63.663843 198.187419 \nL 66.738223 196.255495 \nL 69.812603 194.289866 \nL 72.886983 192.358003 \nL 75.961364 190.42608 \nL 79.035744 188.460465 \nL 82.110124 186.528632 \nL 85.184504 184.596734 \nL 88.258884 182.63113 \nL 91.333264 180.699297 \nL 94.407645 178.767399 \nL 97.482025 176.801796 \nL 100.556405 174.869962 \nL 103.630785 172.938064 \nL 106.705165 170.972461 \nL 109.779545 169.040628 \nL 112.853926 167.10873 \nL 115.928306 165.143127 \nL 119.002686 163.211293 \nL 122.077066 161.279334 \nL 125.151446 159.313621 \nL 128.225826 157.381681 \nL 131.300207 155.449677 \nL 134.374587 153.483964 \nL 137.448967 151.552024 \nL 140.523347 149.620019 \nL 143.597727 147.654307 \nL 146.672107 145.722367 \nL 149.746488 143.790362 \nL 152.820868 141.824649 \nL 155.895248 139.892709 \nL 158.969628 137.960705 \nL 162.044008 135.994992 \nL 165.118388 134.063052 \nL 168.192769 132.131048 \nL 171.267149 130.165335 \nL 174.341529 128.233395 \nL 177.415909 126.301391 \nL 180.490289 124.335678 \nL 183.564669 122.403738 \nL 186.63905 120.471733 \nL 189.71343 118.50602 \nL 192.78781 116.574081 \nL 195.86219 114.642076 \nL 198.93657 112.676363 \nL 202.01095 110.744423 \nL 205.085331 108.812419 \nL 208.159711 106.846996 \nL 211.234091 104.915418 \nL 214.308471 102.983788 \nL 217.382851 101.018468 \nL 220.457231 99.08689 \nL 223.531612 97.155259 \nL 226.605992 95.18994 \nL 229.680372 93.258361 \nL 232.754752 91.326731 \nL 235.829132 89.361412 \nL 238.903512 87.429833 \nL 241.977893 85.498203 \nL 245.052273 83.532884 \nL 248.126653 81.601305 \nL 251.201033 79.669675 \nL 254.275413 77.704355 \nL 257.349793 75.772777 \nL 260.424174 73.841147 \nL 263.498554 71.875827 \nL 266.572934 69.944249 \nL 269.647314 68.012618 \nL 272.721694 66.047299 \nL 275.796074 64.11572 \nL 278.870455 62.18409 \nL 281.944835 60.218771 \nL 285.019215 58.287192 \nL 288.093595 56.355562 \nL 291.167975 54.390243 \nL 294.242355 52.458664 \nL 297.316736 50.527034 \nL 300.391116 48.561714 \nL 303.465496 46.630136 \nL 306.539876 44.698506 \nL 309.614256 42.733186 \nL 312.688636 40.801608 \nL 315.763017 38.869977 \nL 318.837397 36.904658 \nL 321.911777 34.973079 \nL 324.986157 33.041449 \nL 328.060537 31.07613 \nL 331.134917 29.144551 \nL 334.209298 27.212921 \nL 337.283678 25.247602 \nL 340.358058 23.316023 \nL 343.432438 21.384393 \nL 346.506818 19.419073 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path clip-path=\"url(#pa8c7e465a9)\" d=\"M 42.143182 204.609172 \nL 45.217562 202.757275 \nL 48.291942 200.810906 \nL 51.366322 199.006772 \nL 54.440702 197.198893 \nL 57.515083 195.391014 \nL 60.589463 193.583135 \nL 63.663843 191.696906 \nL 66.738223 189.805212 \nL 69.812603 187.889254 \nL 72.886983 186.003029 \nL 75.961364 184.111355 \nL 79.035744 182.195399 \nL 82.110124 180.309174 \nL 85.184504 178.4175 \nL 88.258884 176.501544 \nL 91.333264 174.615319 \nL 94.407645 172.723645 \nL 97.482025 170.807689 \nL 100.556405 168.921464 \nL 103.630785 167.02979 \nL 106.705165 165.113834 \nL 109.779545 163.227609 \nL 112.853926 161.3359 \nL 115.928306 159.419876 \nL 119.002686 157.533586 \nL 122.077066 155.641851 \nL 125.151446 153.725827 \nL 128.225826 151.839537 \nL 131.300207 149.947803 \nL 134.374587 148.031779 \nL 137.448967 146.145489 \nL 140.523347 144.253754 \nL 143.597727 142.33773 \nL 146.672107 140.45144 \nL 149.746488 138.559706 \nL 152.820868 136.643682 \nL 155.895248 134.757392 \nL 158.969628 132.865657 \nL 162.044008 130.949633 \nL 165.118388 129.063343 \nL 168.192769 127.171609 \nL 171.267149 125.255585 \nL 174.341529 123.369295 \nL 177.415909 121.47756 \nL 180.490289 119.561536 \nL 183.564669 117.675246 \nL 186.63905 115.783512 \nL 189.71343 113.867487 \nL 192.78781 111.981198 \nL 195.86219 110.089463 \nL 198.93657 108.173445 \nL 202.01095 106.287465 \nL 205.085331 104.396028 \nL 208.159711 102.480332 \nL 211.234091 100.594352 \nL 214.308471 98.702914 \nL 217.382851 96.787219 \nL 220.457231 94.901239 \nL 223.531612 93.009801 \nL 226.605992 91.094106 \nL 229.680372 89.208126 \nL 232.754752 87.316688 \nL 235.829132 85.400993 \nL 238.903512 83.515013 \nL 241.977893 81.623575 \nL 245.052273 79.70788 \nL 248.126653 77.8219 \nL 251.201033 75.930462 \nL 254.275413 74.014767 \nL 257.349793 72.128787 \nL 260.424174 70.237349 \nL 263.498554 68.321654 \nL 266.572934 66.435674 \nL 269.647314 64.544236 \nL 272.721694 62.628541 \nL 275.796074 60.742561 \nL 278.870455 58.851123 \nL 281.944835 56.935428 \nL 285.019215 55.049448 \nL 288.093595 53.15801 \nL 291.167975 51.242315 \nL 294.242355 49.356335 \nL 297.316736 47.464897 \nL 300.391116 45.549202 \nL 303.465496 43.663222 \nL 306.539876 41.771784 \nL 309.614256 39.856089 \nL 312.688636 37.970109 \nL 315.763017 36.078671 \nL 318.837397 34.162976 \nL 321.911777 32.276996 \nL 324.986157 30.385558 \nL 328.060537 28.469862 \nL 331.134917 26.583883 \nL 334.209298 24.692445 \nL 337.283678 22.776749 \nL 340.358058 20.890769 \nL 343.432438 18.999332 \nL 346.506818 17.083636 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 361.725 224.64 \nL 361.725 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 361.725 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 361.725 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pa8c7e465a9\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Neuron Label Assignments:\n\t Output Neuron[ 0 ]: tensor(0) Proportions: tensor([0.8595, 0.1405]) Rates: tensor([5000.,  817.])\n\t Output Neuron[ 1 ]: tensor(1) Proportions: tensor([0.1437, 0.8563]) Rates: tensor([ 839., 5000.])\n"
     ]
    }
   ],
   "source": [
    "weight_history = None\n",
    "num_correct = 0.0\n",
    "\n",
    "### DEBUG ###\n",
    "\n",
    "### can be used to assist the network with learning the inputs\n",
    "supervised = True\n",
    "log_messages = False\n",
    "plot_weights = True\n",
    "epochs = 100\n",
    "\n",
    "# iterate for epochs\n",
    "for step in range(epochs):\n",
    "    for sample in encoded_train_inputs:\n",
    "        \n",
    "        # get the label for the current image\n",
    "        labels[0] = sample[\"Label\"]\n",
    "\n",
    "        # randomly decide which output neuron should spike if more than one neuron corresponds to the class\n",
    "        # choice will always be 0 if there is one neuron per output class\n",
    "        choice = np.random.choice(per_class, size=1, replace=False)\n",
    "\n",
    "        # clamp on the output layer forces the node corresponding to the label's class to spike\n",
    "        # this is necessary in order for the network to learn which neurons correspond to which classes\n",
    "        # clamp: Mapping of layer names to boolean masks if neurons should be clamped to spiking. \n",
    "        # The ``Tensor``s have shape ``[n_neurons]`` or ``[time, n_neurons]``.\n",
    "        clamp = {lif_layer_name: per_class * labels[0] + torch.Tensor(choice).long()} if supervised else {}\n",
    "\n",
    "        ### Step 1: Run the network with the provided inputs ###\n",
    "        network.run(inputs=sample[\"Inputs\"], time=time, clamp=clamp)\n",
    "\n",
    "        ### Step 2: Get the spikes produced at the output layer ###\n",
    "        spike_record[0] = layer_monitors[lif_layer_name].get(\"s\").view(time, lif_neurons)\n",
    "        \n",
    "        ### Step 3: ###\n",
    "\n",
    "        # Assign labels to the neurons based on highest average spiking activity.\n",
    "        # Returns a Tuple of class assignments, per-class spike proportions, and per-class firing rates \n",
    "        # Return Type: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "        assignments, proportions, rates = assign_labels( spike_record, labels, n_classes, rates )\n",
    "\n",
    "        ### Step 4: Classify data based on the neuron (label) with the highest average spiking activity ###\n",
    "\n",
    "        # Classify data with the label with highest average spiking activity over all neurons.\n",
    "        all_activity_pred = all_activity(spike_record, assignments, n_classes)\n",
    "\n",
    "        ### Step 5: Classify data based on the neuron (label) with the highest average spiking activity\n",
    "        ###         weighted by class-wise proportion ###\n",
    "        proportion_pred = proportion_weighting(spike_record, assignments, proportions, n_classes)\n",
    "\n",
    "        ### Update Accuracy\n",
    "        num_correct += 1 if (labels.numpy()[0] == all_activity_pred.numpy()[0]) else 0\n",
    "\n",
    "        ######## Display Information ########\n",
    "        if log_messages:\n",
    "            print(\"Actual Label:\",labels.numpy(),\"|\",\"Predicted Label:\",all_activity_pred.numpy(),\"|\",\"Proportionally Predicted Label:\",proportion_pred.numpy())\n",
    "            \n",
    "            print(\"Neuron Label Assignments:\")\n",
    "            for idx in range(assignments.numel()):\n",
    "                print(\n",
    "                    \"\\t Output Neuron[\",idx,\"]:\",assignments[idx],\n",
    "                    \"Proportions:\",proportions[idx],\n",
    "                    \"Rates:\",rates[idx]\n",
    "                    )\n",
    "            print(\"\\n\")\n",
    "        #####################################\n",
    "\n",
    "\n",
    "    ### For Weight Plotting ###\n",
    "    if plot_weights:\n",
    "        weights = network.connections[(\"Input Layer\", \"LIF Layer\")].w[:,0].numpy().reshape((1,input_neurons))\n",
    "        weight_history = weights.copy() if step == 0 else np.concatenate((weight_history,weights),axis=0)\n",
    "    print(\"Neuron 0 Weights:\\n\",network.connections[(\"Input Layer\", \"LIF Layer\")].w[:,0])\n",
    "    print(\"Neuron 1 Weights:\\n\",network.connections[(\"Input Layer\", \"LIF Layer\")].w[:,1])\n",
    "    print(\"====================\")\n",
    "    #############################\n",
    "\n",
    "    if log_messages:\n",
    "        print(\"Epoch #\",step,\"\\tAccuracy:\", num_correct / ((step + 1) * len(encoded_inputs)) )\n",
    "        print(\"===========================\\n\\n\")\n",
    "\n",
    "### For Weight Plotting ###\n",
    "# Plot Weight Changes\n",
    "if plot_weights:\n",
    "    [plt.plot(weight_history[:,idx]) for idx in range(weight_history.shape[1])]\n",
    "    plt.show()\n",
    "    \n",
    "#############################\n",
    "\n",
    "### Print Final Class Assignments and Proportions ###\n",
    "print(\"Neuron Label Assignments:\")\n",
    "for idx in range(assignments.numel()):\n",
    "    print(\n",
    "        \"\\t Output Neuron[\",idx,\"]:\",assignments[idx],\n",
    "        \"Proportions:\",proportions[idx],\n",
    "        \"Rates:\",rates[idx]\n",
    "        )"
   ]
  },
  {
   "source": [
    "### 4.6 Evaluate Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Actual Label: [1] | Predicted Label: [0] | Proportionally Predicted Label: [0]\n",
      "Neuron Label Assignments:\n",
      "\t Output Neuron[ 0 ]: tensor(0) Proportions: tensor([0.8593, 0.1407]) Rates: tensor([5000.,  819.])\n",
      "\t Output Neuron[ 1 ]: tensor(1) Proportions: tensor([0.1436, 0.8564]) Rates: tensor([ 839., 5002.])\n",
      "\n",
      "\n",
      "Actual Label: [1] | Predicted Label: [1] | Proportionally Predicted Label: [1]\n",
      "Neuron Label Assignments:\n",
      "\t Output Neuron[ 0 ]: tensor(0) Proportions: tensor([0.8591, 0.1409]) Rates: tensor([5000.,  820.])\n",
      "\t Output Neuron[ 1 ]: tensor(1) Proportions: tensor([0.1436, 0.8564]) Rates: tensor([ 839., 5004.])\n",
      "\n",
      "\n",
      "Actual Label: [1] | Predicted Label: [0] | Proportionally Predicted Label: [0]\n",
      "Neuron Label Assignments:\n",
      "\t Output Neuron[ 0 ]: tensor(0) Proportions: tensor([0.8588, 0.1412]) Rates: tensor([5000.,  822.])\n",
      "\t Output Neuron[ 1 ]: tensor(1) Proportions: tensor([0.1436, 0.8564]) Rates: tensor([ 839., 5005.])\n",
      "\n",
      "\n",
      "Actual Label: [0] | Predicted Label: [0] | Proportionally Predicted Label: [0]\n",
      "Neuron Label Assignments:\n",
      "\t Output Neuron[ 0 ]: tensor(0) Proportions: tensor([0.8589, 0.1411]) Rates: tensor([5002.,  822.])\n",
      "\t Output Neuron[ 1 ]: tensor(1) Proportions: tensor([0.1439, 0.8561]) Rates: tensor([ 841., 5005.])\n",
      "\n",
      "\n",
      "Actual Label: [0] | Predicted Label: [1] | Proportionally Predicted Label: [1]\n",
      "Neuron Label Assignments:\n",
      "\t Output Neuron[ 0 ]: tensor(0) Proportions: tensor([0.8589, 0.1411]) Rates: tensor([5003.,  822.])\n",
      "\t Output Neuron[ 1 ]: tensor(1) Proportions: tensor([0.1442, 0.8558]) Rates: tensor([ 843., 5005.])\n",
      "\n",
      "\n",
      "Actual Label: [0] | Predicted Label: [0] | Proportionally Predicted Label: [0]\n",
      "Neuron Label Assignments:\n",
      "\t Output Neuron[ 0 ]: tensor(0) Proportions: tensor([0.8589, 0.1411]) Rates: tensor([5005.,  822.])\n",
      "\t Output Neuron[ 1 ]: tensor(1) Proportions: tensor([0.1443, 0.8557]) Rates: tensor([ 844., 5005.])\n",
      "\n",
      "\n",
      "Actual Label: [0] | Predicted Label: [0] | Proportionally Predicted Label: [0]\n",
      "Neuron Label Assignments:\n",
      "\t Output Neuron[ 0 ]: tensor(0) Proportions: tensor([0.8590, 0.1410]) Rates: tensor([5007.,  822.])\n",
      "\t Output Neuron[ 1 ]: tensor(1) Proportions: tensor([0.1446, 0.8554]) Rates: tensor([ 846., 5005.])\n",
      "\n",
      "\n",
      "Actual Label: [1] | Predicted Label: [1] | Proportionally Predicted Label: [1]\n",
      "Neuron Label Assignments:\n",
      "\t Output Neuron[ 0 ]: tensor(0) Proportions: tensor([0.8588, 0.1412]) Rates: tensor([5007.,  823.])\n",
      "\t Output Neuron[ 1 ]: tensor(1) Proportions: tensor([0.1445, 0.8555]) Rates: tensor([ 846., 5007.])\n",
      "\n",
      "\n",
      "Actual Label: [0] | Predicted Label: [0] | Proportionally Predicted Label: [0]\n",
      "Neuron Label Assignments:\n",
      "\t Output Neuron[ 0 ]: tensor(0) Proportions: tensor([0.8589, 0.1411]) Rates: tensor([5009.,  823.])\n",
      "\t Output Neuron[ 1 ]: tensor(1) Proportions: tensor([0.1447, 0.8553]) Rates: tensor([ 847., 5007.])\n",
      "\n",
      "\n",
      "Actual Label: [1] | Predicted Label: [0] | Proportionally Predicted Label: [0]\n",
      "Neuron Label Assignments:\n",
      "\t Output Neuron[ 0 ]: tensor(0) Proportions: tensor([0.8586, 0.1414]) Rates: tensor([5009.,  825.])\n",
      "\t Output Neuron[ 1 ]: tensor(1) Proportions: tensor([0.1446, 0.8554]) Rates: tensor([ 847., 5009.])\n",
      "\n",
      "\n",
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "\n",
    "log_messages = True\n",
    "\n",
    "# disable training mode\n",
    "network.train(False)\n",
    "\n",
    "# loop through each test example and record performance\n",
    "for sample in encoded_test_inputs:\n",
    "\n",
    "    # get the label for the current image\n",
    "    labels[0] = sample[\"Label\"]\n",
    "\n",
    "    ### Step 1: Run the network with the provided inputs ###\n",
    "    network.run(inputs=sample[\"Inputs\"], time=time)\n",
    "\n",
    "    ### Step 2: Get the spikes produced at the output layer ###\n",
    "    spike_record[0] = layer_monitors[lif_layer_name].get(\"s\").view(time, lif_neurons)\n",
    "\n",
    "    ### Step 3: ###\n",
    "\n",
    "    # Assign labels to the neurons based on highest average spiking activity.\n",
    "    # Returns a Tuple of class assignments, per-class spike proportions, and per-class firing rates \n",
    "    # Return Type: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "    assignments, proportions, rates = assign_labels( spike_record, labels, n_classes, rates )\n",
    "\n",
    "    ### Step 4: Classify data based on the neuron (label) with the highest average spiking activity ###\n",
    "\n",
    "    # Classify data with the label with highest average spiking activity over all neurons.\n",
    "    all_activity_pred = all_activity(spike_record, assignments, n_classes)\n",
    "\n",
    "    ### Step 5: Classify data based on the neuron (label) with the highest average spiking activity\n",
    "    ###         weighted by class-wise proportion ###\n",
    "    proportion_pred = proportion_weighting(spike_record, assignments, proportions, n_classes)\n",
    "\n",
    "    ### Update Accuracy\n",
    "    num_correct += 1 if (labels.numpy()[0] == all_activity_pred.numpy()[0]) else 0\n",
    "\n",
    "    ######## Display Information ########\n",
    "    if log_messages:\n",
    "        print(\"Actual Label:\",labels.numpy(),\"|\",\"Predicted Label:\",all_activity_pred.numpy(),\"|\",\"Proportionally Predicted Label:\",proportion_pred.numpy())\n",
    "        \n",
    "        print(\"Neuron Label Assignments:\")\n",
    "        for idx in range(assignments.numel()):\n",
    "            print(\n",
    "                \"\\t Output Neuron[\",idx,\"]:\",assignments[idx],\n",
    "                \"Proportions:\",proportions[idx],\n",
    "                \"Rates:\",rates[idx]\n",
    "                )\n",
    "        print(\"\\n\")\n",
    "    #####################################\n",
    "print(\"Accuracy:\", num_correct / len(encoded_test_inputs) )"
   ]
  },
  {
   "source": [
    "## 5. Learning Rules\n",
    "\n",
    "### 5a. PostPre\n",
    "\n",
    "Simple STDP rule involving both pre- and post-synaptic spiking activity. By default, pre-synaptic update is negative and the post-synaptic update is positive.\n",
    "\n",
    "| Parameters   | Type                                    | Description                                                                               | Default Value |\n",
    "|--------------|-----------------------------------------|-------------------------------------------------------------------------------------------|---------------|\n",
    "| connection   | AbstractConnection                      | An `AbstractConnection` object whose weights the `PostPre` learning rule will modify. |               |\n",
    "| nu           | Optional\\[Union\\[float, Sequence\\[float]]] | Single or pair of learning rates for pre- and post-synaptic events.                       | None          |\n",
    "| reduction    | Optional\\[callable]                      | Method for reducing parameter updates along the batch                                     | None          |\n",
    "| weight_decay | float                                   | Constant multiple to decay weights by on each iteration.                                  | 0.0           |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6. Custom Learning Rules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Custom learning rules can be implemented by subclassing `bindsnet.learning.LearningRule` and providing implementations for the types of `AbstractConnection` objects intended to be used. \n",
    "\n",
    "For example, the `Connection` and `LocalConnection` objects rely on the implementation of a private method, `_connection_update`, whereas the `Conv2dConnection` object uses the `_conv2d_connection_update` version."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}