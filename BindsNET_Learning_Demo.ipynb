{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# BindsNET Learning Techniques\n",
    "\n",
    "## 1. Table of Contents\n",
    "1. Table of Contents\n",
    "2. Overview\n",
    "3. Import Statements\n",
    "4. Learning Flow\n",
    "    1. Define Simulation Parameters\n",
    "    2. Create Input Data\n",
    "    3. Configure Network Architecture\n",
    "    4. Define Simulation Variables\n",
    "    5. Perform Learning Iterations\n",
    "    6. Evaluate Classification Performance\n",
    "5. Learning Rules\n",
    "    1. PostPre\n",
    "    2. Hebbian\n",
    "    3. WeightDependentPostPre\n",
    "    4. MSTDP\n",
    "    5. MSTDPET\n",
    "    6. RMAX\n",
    "6. Custom Learning Rules\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Overview\n",
    "\n",
    "Detail documentation of usage of learning rules has been specified [here](https://bindsnet-docs.readthedocs.io/guide/guide_part_ii.html). This document will go into more specific examples of configuring a spiking neural network in BindsNET."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The specified learning rule is passed into a `Connection` object via the `update_rule` argument. The connection encapsulates the learning rule object.\n",
    "\n",
    "* `nu`: a 2-tuple pre- and post- synaptic learning rates (how quickly synapse weights change)\n",
    "* `reduction`: specifies how parameter updates are aggregated across the batch dimension\n",
    "* `weight_decay`: specifies the time constant of the rate of decay of synapse weights to zero\n",
    "\n",
    "Parameter updates are averaged across the batch dimension by default, so there is no weight decay.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from bindsnet.network.nodes import Input, LIFNodes\n",
    "from bindsnet.network.topology import Connection\n",
    "from bindsnet.learning import PostPre\n",
    "\n",
    "# Create two populations of neurons, one to act as the \"source\"\n",
    "# population, and the other, the \"target population\".\n",
    "# Neurons involved in certain learning rules must record synaptic\n",
    "# traces, a vector of short-term memories of the last emitted spikes.\n",
    "source_layer = Input(n=100, traces=True)\n",
    "target_layer = LIFNodes(n=1000, traces=True)\n",
    "\n",
    "# Connect the two layers.\n",
    "connection = Connection(\n",
    "    source=source_layer, target=target_layer, update_rule=PostPre, nu=(1e-4, 1e-2)\n",
    ")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bindsnet'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5fe0f6184214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbindsnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLIFNodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbindsnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbindsnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPostPre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create two populations of neurons, one to act as the \"source\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bindsnet'"
     ]
    }
   ]
  },
  {
   "source": [
    "## 3. Import Statements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from bindsnet.encoding import *\n",
    "from bindsnet.network import Network\n",
    "from bindsnet.network.monitors import Monitor\n",
    "from bindsnet.network.monitors import NetworkMonitor\n",
    "\n",
    "from bindsnet.analysis.plotting import plot_spikes, plot_voltages, plot_input, plot_weights\n",
    "\n",
    "from bindsnet.network.nodes import Input, LIFNodes\n",
    "from bindsnet.network.topology import Connection\n",
    "from bindsnet.learning import PostPre, Hebbian, WeightDependentPostPre, MSTDP, MSTDPET\n",
    "\n",
    "from bindsnet.evaluation import all_activity, proportion_weighting, assign_labels\n",
    "from bindsnet.utils import get_square_weights, get_square_assignments"
   ]
  },
  {
   "source": [
    "## 4. Learning Flow\n",
    "\n",
    "1. Define Simulation Parameters\n",
    "2. Create Input Data\n",
    "3. Configure Network Architecture\n",
    "4. Define Simulation Variables\n",
    "5. Perform Learning Iterations\n",
    "6. Evaluate Classification Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.1 Simulation Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input Data Parameters ###\n",
    "\n",
    "# number of training samples\n",
    "training_samples = 1\n",
    "testing_samples = 10\n",
    "\n",
    "# set number of classes\n",
    "n_classes = 2\n",
    "\n",
    "### Network Configuration Parameters ###\n",
    "\n",
    "# configure number of input neurons\n",
    "input_layer_name = \"Input Layer\"\n",
    "input_neurons = 9\n",
    "\n",
    "# configure the number of output lif neurons\n",
    "lif_layer_name = \"LIF Layer\"\n",
    "lif_neurons = 2\n",
    "\n",
    "### Simulation Parameters ###\n",
    "\n",
    "# simulation time\n",
    "time = 10\n",
    "dt = 1\n",
    "\n",
    "# number of training iterations\n",
    "epochs = 1\n",
    "\n",
    "# ratio of neurons to classes\n",
    "per_class = int(lif_neurons / n_classes)"
   ]
  },
  {
   "source": [
    "### 4.2 Input Configuration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store unique images in a list\n",
    "imgs = []\n",
    "\n",
    "# Class 0 Image\n",
    "img0 = {\"Label\" : 0, \"Image\" : torch.FloatTensor([[1,1,1],[1,0,1],[1,1,1]])}\n",
    "imgs.append(img0)\n",
    "\n",
    "# Class 1 Image\n",
    "img1 = {\"Label\" : 1, \"Image\" : torch.FloatTensor([[0,1,0],[0,1,0],[0,1,0]])}\n",
    "imgs.append(img1)\n",
    "\n",
    "# initialize list of inputs for training\n",
    "training_dataset = []\n",
    "\n",
    "# for the number of specified training samples\n",
    "for i in range(training_samples):\n",
    "\n",
    "    # randomly select a training sample\n",
    "    # rand_sample = random.randint(0,n_classes-1)\n",
    "    \n",
    "    # provide an even number of training samples\n",
    "    rand_sample = i % n_classes\n",
    "\n",
    "    # add the sample to the list of training samples\n",
    "    training_dataset.append(imgs[rand_sample])\n",
    "\n",
    "# initialize the encoder\n",
    "encoder = BernoulliEncoder(time=time, dt=dt)\n",
    "\n",
    "# list of encoded images for random selection during training\n",
    "encoded_train_inputs = []\n",
    "\n",
    "# loop through encode each image type and store into a list of encoded images\n",
    "for sample in training_dataset:\n",
    "\n",
    "    # encode the image \n",
    "    encoded_img = encoder(torch.flatten(sample[\"Image\"]))\n",
    "\n",
    "    # encoded image input for the network\n",
    "    encoded_img_input = {input_layer_name: encoded_img}\n",
    "\n",
    "    # encoded image label\n",
    "    encoded_img_label = sample[\"Label\"]\n",
    "\n",
    "    # add to the encoded input list along with the input layer name\n",
    "    encoded_train_inputs.append({\"Label\" : encoded_img_label, \"Inputs\" : encoded_img_input})\n",
    "\n",
    "# initialize list of inputs for testing\n",
    "testing_dataset = []\n",
    "\n",
    "# for the number of specified testing samples\n",
    "for i in range(testing_samples):\n",
    "\n",
    "    # randomly select a training sample\n",
    "    rand_sample = random.randint(0,n_classes-1)\n",
    "\n",
    "    # add the sample to the list of training samples\n",
    "    testing_dataset.append(imgs[rand_sample])\n",
    "\n",
    "# list of encoded images for random selection during training\n",
    "encoded_test_inputs = []\n",
    "\n",
    "# loop through encode each image type and store into a list of encoded images\n",
    "for sample in testing_dataset:\n",
    "\n",
    "    # encode the image \n",
    "    encoded_img = encoder(torch.flatten(sample[\"Image\"]))\n",
    "\n",
    "    # encoded image input for the network\n",
    "    encoded_img_input = {input_layer_name: encoded_img}\n",
    "\n",
    "    # encoded image label\n",
    "    encoded_img_label = sample[\"Label\"]\n",
    "\n",
    "    # add to the encoded input list along with the input layer name\n",
    "    encoded_test_inputs.append({\"Label\" : encoded_img_label, \"Inputs\" : encoded_img_input})"
   ]
  },
  {
   "source": [
    "### 4.3 Network Configuration\n",
    "\n",
    "When creating a connection between two layers, the learning (update) rule should be specified as well as the learning rate (nu) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network\n",
    "network = Network()\n",
    "\n",
    "# configure weights for the synapses between the input layer and LIF layer\n",
    "#w = torch.round(torch.abs(2 * torch.randn(input_neurons, lif_neurons)))\n",
    "w = torch.zeros(input_neurons,lif_neurons)\n",
    "\n",
    "# initialize input and LIF layers\n",
    "# spike traces must be recorded (why?)\n",
    "\n",
    "# initialize input layer\n",
    "input_layer = Input(n=input_neurons,traces=True)\n",
    "\n",
    "# initialize input layer\n",
    "lif_layer = LIFNodes(n=lif_neurons,traces=True)\n",
    "\n",
    "# initialize connection between the input layer and the LIF layer\n",
    "# specify the learning (update) rule and learning rate (nu)\n",
    "connection = Connection(\n",
    "    #source=input_layer, target=lif_layer, w=w, update_rule=PostPre, nu=(1e-4, 1e-2)\n",
    "    source=input_layer, target=lif_layer, w=w, update_rule=PostPre, nu=(1, 1)\n",
    ")\n",
    "\n",
    "# add input layer to the network\n",
    "network.add_layer(\n",
    "    layer=input_layer, name=input_layer_name\n",
    ")\n",
    "\n",
    "# add lif neuron layer to the network\n",
    "network.add_layer(\n",
    "    layer=lif_layer, name=lif_layer_name\n",
    ")\n",
    "\n",
    "# add connection to network\n",
    "network.add_connection(\n",
    "    connection=connection, source=input_layer_name, target=lif_layer_name\n",
    ")"
   ]
  },
  {
   "source": [
    "### 4.4 Simulation Variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the spike times of each neuron during the simulation.\n",
    "spike_record = torch.zeros(1, int(time / dt), lif_neurons)\n",
    "\n",
    "# record the mapping of each neuron to its corresponding label\n",
    "assignments = -torch.ones_like(torch.Tensor(lif_neurons))\n",
    "\n",
    "# how frequently each neuron fires for each input class\n",
    "rates = torch.zeros_like(torch.Tensor(lif_neurons, n_classes))\n",
    "\n",
    "# the likelihood of each neuron firing for each input class\n",
    "proportions = torch.zeros_like(torch.Tensor(lif_neurons, n_classes))\n",
    "\n",
    "\n",
    "# label(s) of the input(s) being processed\n",
    "labels = torch.empty(1,dtype=torch.int)\n",
    "\n",
    "# create a spike monitor for each layer in the network\n",
    "# this allows us to read the spikes in order to assign labels to neurons and determine the predicted class \n",
    "layer_monitors = {}\n",
    "for layer in set(network.layers):\n",
    "\n",
    "    # initialize spike monitor at the layer\n",
    "    # do not record the voltage if at the input layer\n",
    "    state_vars = [\"s\",\"v\"] if (layer != input_layer_name) else [\"s\"]\n",
    "    layer_monitors[layer] = Monitor(network.layers[layer], state_vars=state_vars, time=int(time/dt))\n",
    "\n",
    "    # connect the monitor to the network\n",
    "    network.add_monitor(layer_monitors[layer], name=\"%s_spikes\" % layer)"
   ]
  },
  {
   "source": [
    "### 4.5 Training\n",
    "\n",
    "Below are descriptions of the functions for evaluating the behavior of an SNN in BindsNET\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "`all_activity()`\n",
    "\n",
    "Classify data with the label with highest average spiking activity over all neurons.\n",
    "\n",
    "Returns a predictions tensor of shape `(n_samples,)` resulting from the \"all activity\" classification scheme (`torch.Tensor`)\n",
    "\n",
    "| Parameter  | Type         | Description                                                                           | Default Value |\n",
    "|-------------|--------------|---------------------------------------------------------------------------------------|---------|\n",
    "| spikes      | `torch.Tensor` | Binary tensor of shape `(n_samples, time, n_neurons)` of a layer'sspiking activity. |         |\n",
    "| assignments | `torch.Tensor` | A vector of shape `(n_neurons,)` of neuron label assignments.                       |         |\n",
    "| n_labels    | `int`          | The number of target labels in the data.                                              |         |\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "`proportion_weighting()`\n",
    "\n",
    "Classify data with the label with highest average spiking activity over all neurons, weighted by class-wise proportion.\n",
    "\n",
    "Returns a predictions tensor of shape `(n_samples,)` resulting from the \"proportion weighting\" classification scheme (`torch.Tensor`)\n",
    "\n",
    "| Parameter   | Type         | Description                                                                                              | Default Value |\n",
    "|-------------|--------------|----------------------------------------------------------------------------------------------------------|---------------|\n",
    "| spikes      | `torch.Tensor` | Binary tensor of shape `(n_samples, time, n_neurons)` of a single layer's spiking activity.            |               |\n",
    "| assignments | `torch.Tensor` | A vector of shape `(n_neurons,)` of neuron label assignments.                                          |               |\n",
    "| proportions | `torch.Tensor` | A matrix of shape `(n_neurons, n_labels)` giving the per-class proportions of neuron spiking activity. |               |\n",
    "| n_labels    | `int`          | The number of target labels in the data.                                                                 |               |\n",
    "\n",
    "----\n",
    "\n",
    "`assign_labels()`\n",
    "\n",
    "Assign labels to the neurons based on highest average spiking activity.\n",
    "\n",
    "Returns a Tuple of class assignments, per-class spike proportions, and per-class firing rates (`Tuple[torch.Tensor, torch.Tensor, torch.Tensor]`)\n",
    "\n",
    "| Parameter | Type                     | Descriptions                                                                                  | Default Value |  \n",
    "|------------|--------------------------|-----------------------------------------------------------------------------------------------|---------------|\n",
    "| spikes     | `torch.Tensor`             | Binary tensor of shape `(n_samples, time, n_neurons)` of a single layer's spiking activity. |                | \n",
    "| labels     | `torch.Tensor`             | Vector of shape `(n_samples,)` with data labels corresponding to spiking activity.          |                | \n",
    "| n_labels   | `int`                      | The number of target labels in the data.                                                      |                | \n",
    "| rates      | `Optional[torch.Tensor]` | If passed, these represent spike rates from a previous `assign_labels()` call.              | None          | \n",
    "| alpha      | `float`                    | Rate of decay of label assignments.                                                           | 1             | \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_history = None\n",
    "num_correct = 0.0\n",
    "\n",
    "### DEBUG ###\n",
    "### can be used to force the network to learn the inputs in a specific way\n",
    "supervised = True\n",
    "### used to determine if status messages are printed out at each sample\n",
    "log_messages = True\n",
    "### used to show weight changes\n",
    "plot_weights = True\n",
    "###############\n",
    "\n",
    "# iterate for epochs\n",
    "for step in range(epochs):\n",
    "    for sample in encoded_train_inputs:\n",
    "        \n",
    "        # get the label for the current image\n",
    "        labels[0] = sample[\"Label\"]\n",
    "\n",
    "        # randomly decide which output neuron should spike if more than one neuron corresponds to the class\n",
    "        # choice will always be 0 if there is one neuron per output class\n",
    "        choice = np.random.choice(per_class, size=1, replace=False)\n",
    "\n",
    "        # clamp on the output layer forces the node corresponding to the label's class to spike\n",
    "        # this is necessary in order for the network to learn which neurons correspond to which classes\n",
    "        # clamp: Mapping of layer names to boolean masks if neurons should be clamped to spiking. \n",
    "        # The ``Tensor``s have shape ``[n_neurons]`` or ``[time, n_neurons]``.\n",
    "        clamp = {lif_layer_name: per_class * labels[0] + torch.Tensor(choice).long()} if supervised else {}\n",
    "\n",
    "        print(sample[\"Inputs\"])\n",
    "\n",
    "        ### Step 1: Run the network with the provided inputs ###\n",
    "        network.run(inputs=sample[\"Inputs\"], time=time, clamp=clamp)\n",
    "\n",
    "        ### Step 2: Get the spikes produced at the output layer ###\n",
    "        spike_record[0] = layer_monitors[lif_layer_name].get(\"s\").view(time, lif_neurons)\n",
    "        \n",
    "        ### Step 3: ###\n",
    "\n",
    "        # Assign labels to the neurons based on highest average spiking activity.\n",
    "        # Returns a Tuple of class assignments, per-class spike proportions, and per-class firing rates \n",
    "        # Return Type: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "        assignments, proportions, rates = assign_labels( spike_record, labels, n_classes, rates )\n",
    "\n",
    "        ### Step 4: Classify data based on the neuron (label) with the highest average spiking activity ###\n",
    "\n",
    "        # Classify data with the label with highest average spiking activity over all neurons.\n",
    "        all_activity_pred = all_activity(spike_record, assignments, n_classes)\n",
    "\n",
    "        ### Step 5: Classify data based on the neuron (label) with the highest average spiking activity\n",
    "        ###         weighted by class-wise proportion ###\n",
    "        proportion_pred = proportion_weighting(spike_record, assignments, proportions, n_classes)\n",
    "\n",
    "        ### Update Accuracy\n",
    "        num_correct += 1 if (labels.numpy()[0] == all_activity_pred.numpy()[0]) else 0\n",
    "\n",
    "        ######## Display Information ########\n",
    "        if log_messages:\n",
    "            print(\"Actual Label:\",labels.numpy(),\"|\",\"Predicted Label:\",all_activity_pred.numpy(),\"|\",\"Proportionally Predicted Label:\",proportion_pred.numpy())\n",
    "            \n",
    "            print(\"Neuron Label Assignments:\")\n",
    "            for idx in range(assignments.numel()):\n",
    "                print(\n",
    "                    \"\\t Output Neuron[\",idx,\"]:\",assignments[idx],\n",
    "                    \"Proportions:\",proportions[idx],\n",
    "                    \"Rates:\",rates[idx]\n",
    "                    )\n",
    "            print(\"\\n\")\n",
    "        #####################################\n",
    "\n",
    "\n",
    "    ### For Weight Plotting ###\n",
    "    if plot_weights:\n",
    "        weights = network.connections[(\"Input Layer\", \"LIF Layer\")].w[:,0].numpy().reshape((1,input_neurons))\n",
    "        weight_history = weights.copy() if step == 0 else np.concatenate((weight_history,weights),axis=0)\n",
    "        print(\"Neuron 0 Weights:\\n\",network.connections[(\"Input Layer\", \"LIF Layer\")].w[:,0])\n",
    "        print(\"Neuron 1 Weights:\\n\",network.connections[(\"Input Layer\", \"LIF Layer\")].w[:,1])\n",
    "        print(\"====================\")\n",
    "    #############################\n",
    "\n",
    "    if log_messages:\n",
    "        print(\"Epoch #\",step,\"\\tAccuracy:\", num_correct / ((step + 1) * len(encoded_train_inputs)) )\n",
    "        print(\"===========================\\n\\n\")\n",
    "\n",
    "### For Weight Plotting ###\n",
    "# Plot Weight Changes\n",
    "if plot_weights:\n",
    "    [plt.plot(weight_history[:,idx]) for idx in range(weight_history.shape[1])]\n",
    "    plt.show()\n",
    "    \n",
    "#############################\n",
    "\n",
    "### Print Final Class Assignments and Proportions ###\n",
    "print(\"Neuron Label Assignments:\")\n",
    "for idx in range(assignments.numel()):\n",
    "    print(\n",
    "        \"\\t Output Neuron[\",idx,\"]:\",assignments[idx],\n",
    "        \"Proportions:\",proportions[idx],\n",
    "        \"Rates:\",rates[idx]\n",
    "        )"
   ]
  },
  {
   "source": [
    "### 4.6 Evaluate Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "\n",
    "log_messages = True\n",
    "\n",
    "# disable training mode\n",
    "network.train(False)\n",
    "\n",
    "# loop through each test example and record performance\n",
    "for sample in encoded_test_inputs:\n",
    "\n",
    "    # get the label for the current image\n",
    "    labels[0] = sample[\"Label\"]\n",
    "\n",
    "    ### Step 1: Run the network with the provided inputs ###\n",
    "    network.run(inputs=sample[\"Inputs\"], time=time)\n",
    "\n",
    "    ### Step 2: Get the spikes produced at the output layer ###\n",
    "    spike_record[0] = layer_monitors[lif_layer_name].get(\"s\").view(time, lif_neurons)\n",
    "\n",
    "    ### Step 3: ###\n",
    "\n",
    "    # Assign labels to the neurons based on highest average spiking activity.\n",
    "    # Returns a Tuple of class assignments, per-class spike proportions, and per-class firing rates \n",
    "    # Return Type: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "    assignments, proportions, rates = assign_labels( spike_record, labels, n_classes, rates )\n",
    "\n",
    "    ### Step 4: Classify data based on the neuron (label) with the highest average spiking activity ###\n",
    "\n",
    "    # Classify data with the label with highest average spiking activity over all neurons.\n",
    "    all_activity_pred = all_activity(spike_record, assignments, n_classes)\n",
    "\n",
    "    ### Step 5: Classify data based on the neuron (label) with the highest average spiking activity\n",
    "    ###         weighted by class-wise proportion ###\n",
    "    proportion_pred = proportion_weighting(spike_record, assignments, proportions, n_classes)\n",
    "\n",
    "    ### Update Accuracy\n",
    "    num_correct += 1 if (labels.numpy()[0] == all_activity_pred.numpy()[0]) else 0\n",
    "\n",
    "    ######## Display Information ########\n",
    "    if log_messages:\n",
    "        print(\"Actual Label:\",labels.numpy(),\"|\",\"Predicted Label:\",all_activity_pred.numpy(),\"|\",\"Proportionally Predicted Label:\",proportion_pred.numpy())\n",
    "        \n",
    "        print(\"Neuron Label Assignments:\")\n",
    "        for idx in range(assignments.numel()):\n",
    "            print(\n",
    "                \"\\t Output Neuron[\",idx,\"]:\",assignments[idx],\n",
    "                \"Proportions:\",proportions[idx],\n",
    "                \"Rates:\",rates[idx]\n",
    "                )\n",
    "        print(\"\\n\")\n",
    "    #####################################\n",
    "print(\"Accuracy:\", num_correct / len(encoded_test_inputs) )"
   ]
  },
  {
   "source": [
    "## 5. Learning Rules\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "#### [Basic STDP Model:](http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity)\n",
    "\n",
    "The weight change $\\Delta w_j$ of a synapse from a presynaptic neuron $j$| depends on the relative timing between presynaptic spike arrivals and postsynaptic spikes. \n",
    "\n",
    "Presynaptic spike arrival times at synapse $j$ are denoted by $t^f_j$ where $f$=1,2,3,... counts the presynaptic spikes. \n",
    "\n",
    "Postsynaptic firing times are denoted by $t^n_i$ where $n$=1,2,3,... counts the postsynaptic spikes. \n",
    "\n",
    "The total weight change $\\Delta w_j$ induced by a stimulation protocol with pairs of pre- and postsynaptic spikes is then:\n",
    "\n",
    "$$\n",
    "\\Delta w = \\sum_{f=1}^{N} \\sum_{n=1}^{N} W (t_i^n - t_j^f)\n",
    "$$\n",
    "\n",
    "where **$W(x)$** denotes one of the STDP functions (also called learning window).\n",
    "\n",
    "A popular choice for the STDP function **$W(x)$**\n",
    "$$\n",
    "W(x)=A_+e^{−x/\\tau+} \\hspace{5mm} for \\hspace{5mm} x > 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "W(x)=−A_−e^{x/\\tau−} \\hspace{5mm} for \\hspace{5mm} x < 0 \n",
    "$$\n",
    "\n",
    "The parameters A+ and A− may depend on the current value of the synaptic weight $w_j$. The time constants are on the order of $\\tau_+$ = 10ms and $\\tau_-$=10ms\n",
    "\n",
    "In summary: \n",
    "\n",
    "The weight change $\\Delta w_j$ will be a decreasing positive value the more $t^n_i$ (post synaptic firing time) exceedes $t^f_j$ (presynaptic spike time). This is also referred to as long-term potentiation (LTP).\n",
    "\n",
    "The weight change $\\Delta w_j$ will be a decreasing negative value the more $t^f_j$ (presynaptic spike time) exceedes $t^n_i$ (post synaptic firing time). This is also referred to as long-term depression (LTD).\n",
    "\n",
    "Source: http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5a. PostPre\n",
    "\n",
    "#### Summary\n",
    "\n",
    "A simple STDP rule involving both pre- and post-synaptic spiking activity. By default, pre-synaptic update is negative and the post-synaptic update is positive.\n",
    "\n",
    "The rule follows the equation below for each timestep $t$:\n",
    "\n",
    "$$\n",
    "\\Delta w (t) = \\eta_1 (e^{\\frac{t - t_{pre}}{\\tau}})S_{post}(t) - \\eta_0 (e^{\\frac{t - t_{post}}{\\tau}})S_{pre}(t)\n",
    "$$\n",
    "\n",
    "Where $S_{pre}(t)$ and $S_{post}(t)$ indicate if there was a spike at time t for either the pre-synaptic or post-synaptic neurons. \n",
    "\n",
    "Additionally $t_{pre}$ is the timestamp when the pre-synaptic neuron last fired, and $t_{post}$ is the timestamp when the post-synaptic neuron last fired.\n",
    "\n",
    "The `trace_decay` value specified when creating a new `Nodes` layer is given by:\n",
    "\n",
    "$$\n",
    "trace\\_decay = {e^{-\\frac{1}{\\tau}}}\n",
    "$$\n",
    "\n",
    "The value $\\Delta w$ is calculated and applied at each synapse for every timestep.\n",
    "\n",
    "#### Table of Parameters\n",
    "\n",
    "| Parameters   | Type                                    | Description                                                                               | Default Value |\n",
    "|--------------|-----------------------------------------|-------------------------------------------------------------------------------------------|---------------|\n",
    "| connection   | AbstractConnection                      | An `AbstractConnection` object whose weights the `PostPre` learning rule will modify. |               |\n",
    "| nu           | Optional\\[Union\\[float, Sequence\\[float]]] | Single or pair of learning rates for pre- and post-synaptic events.                       | None          |\n",
    "| reduction    | Optional\\[callable]                      | Method for reducing parameter updates along the batch                                     | None          |\n",
    "| weight_decay | float                                   | Constant multiple to decay weights by on each iteration.                                  | 0.0           |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5b. WeightDependentPostPre\n",
    "\n",
    "#### Summary\n",
    "\n",
    "STDP rule involving both pre- and post-synaptic spiking activity. The post-synaptic update is positive and the pre- synaptic update is negative, and both are dependent on the magnitude of the synaptic weights.\n",
    "\n",
    "The rule follows the equation below for each timestep $t$:\n",
    "\n",
    "$$\n",
    "\\Delta w (t) = \\eta_1 (e^{\\frac{t - t_{pre}}{\\tau}})S_{post}(t)(w_{max} - w) - \\eta_0 (e^{\\frac{t - t_{post}}{\\tau}})S_{pre}(t) (w - w_{min})\n",
    "$$\n",
    "\n",
    "#### Table of Parameters\n",
    "\n",
    "| Parameters   | Type                                    | Description                                                                               | Default Value |\n",
    "|--------------|-----------------------------------------|-------------------------------------------------------------------------------------------|---------------|\n",
    "| connection   | AbstractConnection                      | An `AbstractConnection` object whose weights the `PostPre` learning rule will modify. |               |\n",
    "| nu           | Optional\\[Union\\[float, Sequence\\[float]]] | Single or pair of learning rates for pre- and post-synaptic events.                       | None          |\n",
    "| reduction    | Optional\\[callable]                      | Method for reducing parameter updates along the batch                                     | None          |\n",
    "| weight_decay | float                                   | Constant multiple to decay weights by on each iteration.                                  | 0.0           |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5c. Hebbian\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Simple Hebbian learning rule. Pre- and post-synaptic updates are both positive.\n",
    "\n",
    "The rule follows the equation below for each timestep $t$:\n",
    "\n",
    "$$\n",
    "\\Delta w (t) = \\eta_1 (e^{\\frac{t - t_{pre}}{\\tau}})S_{post}(t) + \\eta_0 (e^{\\frac{t - t_{post}}{\\tau}})S_{pre}(t)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Table of Parameters\n",
    "\n",
    "| Parameters   | Type                                    | Description                                                                               | Default Value |\n",
    "|--------------|-----------------------------------------|-------------------------------------------------------------------------------------------|---------------|\n",
    "| connection   | AbstractConnection                      | An `AbstractConnection` object whose weights the `PostPre` learning rule will modify. |               |\n",
    "| nu           | Optional\\[Union\\[float, Sequence\\[float]]] | Single or pair of learning rates for pre- and post-synaptic events.                       | None          |\n",
    "| reduction    | Optional\\[callable]                      | Method for reducing parameter updates along the batch                                     | None          |\n",
    "| weight_decay | float                                   | Constant multiple to decay weights by on each iteration.                                  | 0.0           |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5d. MSTDP\n",
    "Reward-modulated STDP. Adapted from [Florian 2007] (https://florian.io/papers/2007_Florian_Modulated_STDP.pdf)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5e. MSTDPET\n",
    "Reward-modulated STDP with eligibility trace. Adapted from [Florian 2007] (https://florian.io/papers/2007_Florian_Modulated_STDP.pdf>)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5f. Rmax\n",
    "Reward-modulated learning rule derived from reward maximization principles. Adapted from [Vasilaki et al., 2009] (https://intranet.physio.unibe.ch/Publikationen/Dokumente/Vasilaki2009PloSComputBio_1.pdf)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6. Custom Learning Rules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Custom learning rules can be implemented by subclassing `bindsnet.learning.LearningRule` and providing implementations for the types of `AbstractConnection` objects intended to be used. \n",
    "\n",
    "For example, the `Connection` and `LocalConnection` objects rely on the implementation of a private method, `_connection_update`, whereas the `Conv2dConnection` object uses the `_conv2d_connection_update` version."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}